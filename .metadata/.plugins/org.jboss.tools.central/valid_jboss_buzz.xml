<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Our advice for configuring Knative Broker for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka" /><author><name>Matthias Wessendorf</name></author><id>04600ac2-35c4-4460-a212-3719a3c5c1e6</id><updated>2023-03-08T07:00:00Z</updated><published>2023-03-08T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn how to set up the &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/"&gt;Knative Broker implementation for Apache Kafka&lt;/a&gt; in a production environment. Recently, the Kafka implementation has been announced as &lt;a href="https://developers.redhat.com/articles/2022/11/15/knative-broker-enhances-kafka-openshift-serverless"&gt;General Availablitly&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One common question for production systems is always the need for an optimal configuration based on the given environment. This article gives recommendations on how to pick a good default configuration for the Knative Broker implementation for Apache Kafka.&lt;/p&gt; &lt;p&gt;The article is based on a small yet production-ready setup of Apache Kafka and Apache Zookeeper, each system consisting of three nodes. You can find a reference Kafka configuration based on &lt;a href="https://strimzi.io/"&gt;Strimzi.io&lt;/a&gt; on the &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/0.32.0/examples/kafka/kafka-persistent.yaml"&gt;Strimzi GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This article is not giving recommendations for the configuration of the Kubernetes cluster.&lt;/p&gt; &lt;h2&gt;Setting up the Knative Broker&lt;/h2&gt; &lt;p&gt;Each broker object uses a Kafka topic for the storage of incoming CloudEvents. The recommendation to start with is as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Partitions: 10&lt;/li&gt; &lt;li&gt;Replication Factor: 3&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Taking this configuration gives you a fault-tolerant, highly-available, and yet scalable basis for your project. Topics are partitioned, meaning they are spread over a number of buckets located on different Kafka brokers. To make your data fault tolerant and highly available, every topic can be replicated, even across geo-regions or datacenters. You can find more detailed information in the &lt;a href="https://kafka.apache.org/intro"&gt;Apache Kafka documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Configuration aspects such as topic partitions or replication factor directly relate to the actual sizing of the cluster. For instance, if your cluster consists of three nodes, you cannot set the replication factor to a higher number, as it directly relates to the available nodes of the Apache Kafkacluster. You can find details about implications in the &lt;a href="https://strimzi.io/documentation/"&gt;Strimzi documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;An example Knative Broker configuration&lt;/h2&gt; &lt;p&gt;Let us take a look at a possible configuration of a Knative Broker for Apache Kafka with this defined cluster in mind:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ConfigMap metadata: name: &lt;broker-name&gt;-config data: bootstrap.servers: &lt;url&gt; auth.secret.ref.name: &lt;optional-secret-name&gt; default.topic.partitions: "10" default.topic.replication.factor: "3" --- apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: Kafka name: &lt;broker-name&gt; spec: config: apiVersion: v1 kind: ConfigMap name: &lt;broker-name&gt;-config delivery: retry: 12 backoffPolicy: exponential backoffDelay: PT0.5S deadLetterSink: ref: apiVersion: eventing.knative.dev/v1 kind: Broker name: &lt;dead-letter-sink-broker-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We see two manifests defined:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A &lt;code&gt;ConfigMap&lt;/code&gt; resource&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Broker&lt;/code&gt; resource.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;code&gt;ConfigMap&lt;/code&gt; defines the URL to the Apache Kafka cluster and references a secret for TLS/SASL &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/#security"&gt;security support&lt;/a&gt;. The manifest also sets the partitions and replication factor for the Kafka topic, internally used for the &lt;code&gt;Broker&lt;/code&gt; object, to store incoming CloudEvents as Kafka records. The &lt;code&gt;Broker&lt;/code&gt; uses the &lt;code&gt;eventing.knative.dev/broker.class&lt;/code&gt; indicating the Kafka-based Knative Broker implementation should be used. On the &lt;code&gt;spec&lt;/code&gt;, it references the &lt;code&gt;ConfigMap&lt;/code&gt;, as well configuration on the broker's event delivery.&lt;/p&gt; &lt;h3&gt;Broker event delivery guarantees and retries&lt;/h3&gt; &lt;p&gt;Knative provides various configuration parameters to control the delivery of events in case of failure. This configuration is able to define a number of retry attempts, a backoff delay, and a backoff policy (linear or exponential). If the event delivery is not successful, the event is delivered to the dead letter sink, if present. The &lt;code&gt;delivery&lt;/code&gt; spec object is global for the given broker object and can be overridden on a per &lt;code&gt;Trigger &lt;/code&gt;basis, if needed.&lt;/p&gt; &lt;p&gt;The following resources offer more details on the &lt;code&gt;delivery&lt;/code&gt; spec:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/knative/specs/blob/main/specs/eventing/data-plane.md"&gt;Knative Eventing Data Plane Contract&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://knative.dev/docs/eventing/event-delivery/#configuring-broker-event-delivery"&gt;Broker event delivery&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;The Knative Broker as a dead letter sink approach&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;deadLetterSink&lt;/code&gt; object can be any &lt;a href="https://knative.dev/docs/concepts/duck-typing/#addressable"&gt;&lt;code&gt;Addressable&lt;/code&gt;&lt;/a&gt; object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. However, this example is configuring a &lt;code&gt;deadLetterSink&lt;/code&gt;, with the different broker object. The benefit of this highly recommended approach is that all undelivered messages are sent to a Knative broker object and consumed from there, using the standard Knative Broker and Trigger APIs.&lt;/p&gt; &lt;h2&gt;An example trigger configuration&lt;/h2&gt; &lt;p&gt;The triggers are used for the egress out of the broker to consuming services. Triggers are executed on the referenced broker object. The following is an example of a trigger configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: &lt;trigger-name&gt; annotations: kafka.eventing.knative.dev/delivery.order: &lt;delivery-order&gt; spec: broker: &lt;broker-name&gt; filter: attributes: type: &lt;cloud-event-type&gt; &lt;ce-extension&gt;: &lt;ce-extension-value&gt; subscriber: ref: apiVersion: v1 kind: Service name: &lt;receiver-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We see a trigger configuration for a specific &lt;code&gt;broker&lt;/code&gt; object, which filters the available CloudEvents, their attributes, and custom CloudEvent extension attributes. Matching events are routed to the &lt;code&gt;subscriber&lt;/code&gt;, which can be any &lt;a href="https://knative.dev/docs/concepts/duck-typing/#addressable"&gt;&lt;code&gt;Addressable&lt;/code&gt;&lt;/a&gt; object that conforms to the Knative Eventing sink contract, as defined above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We recommend applying filters on triggers for CloudEvent attributes and extensions. If no &lt;code&gt;filter&lt;/code&gt; is provided, all occurring CloudEvents are routed to the referenced &lt;code&gt;subscriber&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Message delivery order&lt;/h3&gt; &lt;p&gt;When dispatching events to the &lt;code&gt;subscriber&lt;/code&gt;, the Knative Kafka Broker can be configured to support different delivery ordering guarantees by using the &lt;code&gt;kafka.eventing.knative.dev/delivery.order&lt;/code&gt; annotation on every &lt;code&gt;Trigger&lt;/code&gt; object.&lt;/p&gt; &lt;p&gt;The supported consumer delivery guarantees are:&lt;/p&gt; &lt;ul&gt;&lt;li dir="ltr"&gt;&lt;code&gt;unordered&lt;/code&gt;: An unordered consumer is a non-blocking consumer that delivers messages unordered while preserving proper offset management. This is useful when there is a high demand for parallel consumption and no need for explicit ordering. One example could be the processing of click analytics.&lt;/li&gt; &lt;li dir="ltr"&gt;&lt;code&gt;ordered&lt;/code&gt;: An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. This is useful when there is a need for more strict ordering or if there is a relationship or grouping between events. One example could be the processing of customer orders.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The default ordering guarantee is &lt;code&gt;unordered&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Trigger delivery guarantees and retries&lt;/h3&gt; &lt;p&gt;The global broker delivery configuration can be overridden on a per-trigger basis using the &lt;code&gt;delivery&lt;/code&gt; on the &lt;code&gt;Trigger spec&lt;/code&gt;. The behavior is the same as defined for the Broker event delivery guarantees and retries.&lt;/p&gt; &lt;h2&gt;Our advice for Knative Broker configuration&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how to set up the &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/"&gt;Knative Broker implementation for Apache Kafka&lt;/a&gt; in a production environment. We provided recommendations for picking a good default configuration for the Knative Broker implementation for Apache Kafka. Feel free to comment below if you have questions. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka" title="Our advice for configuring Knative Broker for Apache Kafka"&gt;Our advice for configuring Knative Broker for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Matthias Wessendorf</dc:creator><dc:date>2023-03-08T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.0.0.Alpha5 released - now with Hibernate ORM 6, new Dev UI and more!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-0-alpha5-released/&#xA;            " /><author><name>Max Rydahl Andersen (https://twitter.com/maxandersen)</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-alpha5-released/</id><updated>2023-03-08T00:00:00Z</updated><published>2023-03-08T00:00:00Z</published><summary type="html">We are in the process of getting Quarkus 3 ready - last we got our main branch to Jakarta EE 10 to let us getting more things integrated and tested faster. This time we move from Hibernate 5 to Hibernate 6 - which will cause breakages thus please read the...</summary><dc:creator>Max Rydahl Andersen (https://twitter.com/maxandersen)</dc:creator><dc:date>2023-03-08T00:00:00Z</dc:date></entry><entry><title>6-step tutorial on installing Ansible 2.3 on RHEL 9.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/07/install-ansible-23-on-rhel-91" /><author><name>Tathagata Paul</name></author><id>20d4294e-a08c-4f4b-a9c7-5c78ea545e9e</id><updated>2023-03-07T07:00:00Z</updated><published>2023-03-07T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is an enterprise framework for sharing automation across your organization. This article will demonstrate how to install Ansible Automation Platform 2.3 on a machine running Red Hat Enterprise Linux 9.1.&lt;/p&gt; &lt;h2&gt;6 steps to install Ansible Automation Platform 2.3 on RHEL 9.1&lt;/h2&gt; &lt;p&gt;Follow these quick and easy steps to installation.&lt;/p&gt; &lt;h3&gt;1. Set up prerequisites&lt;/h3&gt; &lt;p&gt;Make sure you have the following prerequisites before starting the installation process:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Red Hat Ansible Automation Platform subscription&lt;/li&gt; &lt;li&gt;Minimum 4GB of RAM&lt;/li&gt; &lt;li&gt;Minimum of 2 CPUs&lt;/li&gt; &lt;li&gt;20GB of dedicated hard disk space&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;2. Download the Ansible Automation Platform&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Download&lt;/a&gt; the Ansible Automation Platform 2.3 and then extract the files. Figure 1 displays the download page.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/figure_1.jpg?itok=UY6V_IRJ" width="1440" height="664" alt="Download page for Ansible Automation Platform" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Ansible Automation Platform software download page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You will see the following files in the extracted directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[user@user ~ ]$ ls bundle  collections  group_vars inventory  licenses  README.md  setup.sh&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before attempting to the install Ansible Automation Platform, update all the system libraries by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf update -y  &amp;&amp; dnf upgrade -y&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3. Configuring the Ansible inventory file&lt;/h3&gt; &lt;p&gt;Configure the credentials for Ansible and PostgreSQL in the Ansible inventory file. The inventory file also contains details about the database.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[automationcontroller] fqdn ansible_connection=local [database] [all:vars] admin_password='redhat' pg_host='' pg_port='' pg_database='awx' pg_username='awx' pg_password='redhat'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update the fields according to the requirements. Then, we will be good to go with our installation.&lt;/p&gt; &lt;h3&gt;4. Running the setup script&lt;/h3&gt; &lt;p&gt;Run the setup.sh file in the extracted folder. There are also various installation scenarios listed &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.3/html/red_hat_ansible_automation_platform_installation_guide/assembly-platform-install-scenario"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo ./setup.sh&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;5. Accessing the Ansible Automation Platform console&lt;/h3&gt; &lt;p&gt;After successfully installing the Ansible Automation Platform 2.3, access the Ansible Automation Platform console via the URL: - &lt;a href="https://localhost/"&gt;https://localhost/&lt;/a&gt;. Log in with the &lt;strong&gt;admin&lt;/strong&gt; username and password set in the inventory file, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure_2_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure_2_0.jpg?itok=xdgnbMcD" width="600" height="522" alt="Ansible Automation Platform Login Page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Ansible Automation Platform login page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;6. Create a subscription allocation&lt;/h3&gt; &lt;p&gt;Create a subscription allocation in the &lt;a href="https://access.redhat.com/management/subscription_allocations"&gt;Red Hat Customer Portal&lt;/a&gt; and export the manifest from the overview page. Upload the manifest to activate your subscription. You can also register for a trial subscription by clicking the &lt;strong&gt;Request Subscription&lt;/strong&gt; button shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure_3.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure_3.jpg?itok=KtziagcH" width="600" height="363" alt="Requesting a subscription for Ansible Automation Platform" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Ansible Subscription page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can explore more ways to import a subscription &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/import_license.html"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 4 shows the Ansible Automation Platform web console.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure_4.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/figure_4.jpg?itok=Z1Up7Y8B" width="1440" height="685" alt="Ansible Automation Platform Web Console" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Ansible Automation Platform 2.3 web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;What's next in your automation journey?&lt;/h2&gt; &lt;p&gt;You can get started with the Ansible Automation Platform by exploring &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;interactive labs at Red Hat Developer&lt;/a&gt;. Ansible Automation Platform is also available as a managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt;Microsoft Azure&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;Amazon Web Services&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/07/install-ansible-23-on-rhel-91" title="6-step tutorial on installing Ansible 2.3 on RHEL 9.1"&gt;6-step tutorial on installing Ansible 2.3 on RHEL 9.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tathagata Paul</dc:creator><dc:date>2023-03-07T07:00:00Z</dc:date></entry><entry><title>5 global environment variables provided by OpenShift GitOps</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/06/5-global-environment-variables-provided-openshift-gitops" /><author><name>Gerald Nunn</name></author><id>c83046ef-a133-418c-81db-a3bdf883e8dc</id><updated>2023-03-06T07:00:00Z</updated><published>2023-03-06T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat OpenShift GitOps provides a compelling out-of-the-box experience for the majority of Red Hat OpenShift customers. However, there are times when organizations have specific requirements to satisfy that require additional tuning. OpenShift GitOps provides a number of global-level environment variables that organizations can apply to tailor their experience.&lt;/p&gt; &lt;h2&gt;5 Environment variables: Overview&lt;/h2&gt; &lt;p&gt;OpenShift GitOps supports the use of environment variables to control operator behavior in specific areas. The following table provides a brief overview of five variables from the &lt;a href="https://github.com/redhat-developer/gitops-operator/blob/master/docs/OpenShift%20GitOps%20Usage%20Guide.md#setting-environment-variables"&gt;&lt;u&gt;upstream documentation&lt;/u&gt;&lt;/a&gt;. Note that this list can change between releases, so it's always a good idea to verify new, deprecated, or removed variables.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="624"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Environment Variable&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ARGOCD_CLUSTER_CONFIG_NAMESPACES&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;OpenShift GitOps instances in the identified namespaces are granted limited additional permissions to manage specific cluster-scoped resources, which include platform operators, optional OLM operators, user management, etc. &lt;p&gt;Multiple namespaces can be specified via a comma delimited list.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CONTROLLER_CLUSTER_ROLE&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;This environment variable enables administrators to configure a common cluster role to use across all managed namespaces in the role bindings the operator creates for the Argo CD application controller.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DISABLE_DEFAULT_ARGOCD_CONSOLELINK&lt;/td&gt; &lt;td&gt;false&lt;/td&gt; &lt;td&gt;When set to `true`, this will disable the ConsoleLink for Argo CD, which appears in the OpenShift Console Application Launcher.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DISABLE_DEFAULT_ARGOCD_INSTANCE&lt;/td&gt; &lt;td&gt;false&lt;/td&gt; &lt;td&gt;When set to `true`, this will disable the default 'ready-to-use' installation of Argo CD in the `openshift-gitops` namespace.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SERVER_CLUSTER_ROLE&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;td&gt;This environment variable enables Administrators to configure a common cluster role to use across all of the managed namespaces in the role bindings the operator creates for the Argo CD server.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;The environment variables can be attached to the subscription object, automatically including them in the operator deployment managed by the subscription. Specifying environment variables in the subscription object is done by including them in spec.config.env as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-gitops-operator namespace: openshift-operators spec: channel: gitops-1.7 config: env: - name: ARGOCD_CLUSTER_CONFIG_NAMESPACES value: openshift-gitops, gitops - name: CONTROLLER_CLUSTER_ROLE value: gitops-controller installPlanApproval: Automatic name: openshift-gitops-operator source: redhat-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;5 Environment variables: Details&lt;/h2&gt; &lt;p&gt;The subsequent sections provide more details about the five environment variables.&lt;/p&gt; &lt;h3&gt;1. ARGOCD_CLUSTER_CONFIG_NAMESPACES&lt;/h3&gt; &lt;p&gt;In OpenShift GitOps, the operator can deploy Argo CD in one of two scopes: cluster or namespace. A cluster-scoped instance is intended to deploy and manage resources across a cluster, whereas namespace scope is limited to a set of specified namespaces. Most importantly, cluster scoped instances have access to cluster-level resources and thus are typically, but not always, used for cluster configuration.&lt;/p&gt; &lt;p&gt;In OpenShift GitOps, the default instance deployed in the openshift-gitops namespace is cluster scoped and, as mentioned previously, intended for cluster configuration. Any other instances deployed are namespace scoped by default and only have access to resources in the namespace to which they are deployed.&lt;/p&gt; &lt;p&gt;To prevent users from deploying Argo CD instances with cluster-level privileges, a cluster administrator must identify the namespaces with cluster privileges by using this environment variable in the subscription object. Since namespace administrators do not have access to the subscription object, this prevents them from elevating the privileges of their own instance and bypassing cluster security.&lt;/p&gt; &lt;p&gt;When an instance is designated as cluster scoped, the operator will automatically create a set of ClusterRole and ClusterRoleBindings for the application controller and server service accounts in that namespace. This default role is not intended to be the equivalent of the standard cluster-admin role. It is given a much smaller set of permissions as follows:&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="NaN"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;&lt;strong&gt;Resource (API Group)&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;strong&gt;What it Manages&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;operators.coreos.com, operator.openshift.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Optional operators managed by OLM&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;user.openshift.io , rbac.authorization.k8s.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Groups, users, and their permissions.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;config.openshift.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Control plane Operators managed by CVO are used to configure cluster-wide build configuration, registry configuration, scheduler policies, etc.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;storage.k8s.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Storage.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;console.openshift.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Console customization.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;machine.openshift.io, machineconfiguration.openshift.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Machine API (machines, MachineSets, machine configuration, etc.)&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;compliance.openshift.io&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Compliance operator, ScanSettingBinding only&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td width="NaN"&gt; &lt;p&gt;Other&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;Namespaces, PV, PVC, and ConfigMaps&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;These permissions can be extended by creating additional ClusterRole/ClusterRoleBinding as needed.&lt;/p&gt; &lt;h3&gt;2. CONTROLLER_CLUSTER_ROLE&lt;/h3&gt; &lt;p&gt;When OpenShift GitOps is deployed in namespace mode, it will create a set of roles and RoleBindings in every namespace that the instance manages to enable the application controller to deploy resources into those namespaces. The cluster administrator indicates these managed namespaces by labeling the namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;argocd.argoproj.io/managed-by: &lt;namespace-of-target-gitops&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The roles that the operator in the namespace creates are namespace scoped and only have access to namespace resources. It cannot perform any actions outside of the namespaces it manages.&lt;/p&gt; &lt;p&gt;While the out-of-the-box role works well for many use cases, organizations may need to modify this role depending on their security requirements, additional resources they want to manage, etc. For example, an organization with security needs driven by regulatory requirements may wish to define a specific set of reduced permissions to meet those requirements.&lt;/p&gt; &lt;p&gt;This environment variable enables the cluster administrator to specify an alternate cluster role instead of the default operator-created role for the application controller. When this variable is provided, the operator will not create a default Role in the namespace but rather only create a RoleBinding in the namespace to the provided cluster role. It is up to the administrator to create this cluster role, having full control over the permissions.&lt;/p&gt; &lt;p&gt;One key aspect to note when defining your own role is that Argo CD will attempt to interact with all resources. When using this feature, you must either provide permission to view/get/watch all resources in your custom cluster role or configure the ArgoCD Custom Resource to include or exclude specific resources via resourceInclusions or resourceExclusions defined in the role.&lt;/p&gt; &lt;p&gt;As an example, in my own installations, I like to specify an alternate ClusterRole that has the following features:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The Kubernetes ClusterRole &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles"&gt;&lt;u&gt;aggregation&lt;/u&gt;&lt;/a&gt; feature allows the cluster role to be easily extended without directly modifying the role.&lt;/li&gt; &lt;li aria-level="1"&gt;The view all permissions role is aggregated into the above ClusterRole.&lt;/li&gt; &lt;li aria-level="1"&gt;The write permissions role incorporates the out-of-the-box admin ClusterRole that OpenShift uses for namespace administrators. The admin is a cluster aggregated role that components like OLM leverage so that when new operators are installed, their resources are automatically aggregated into the role.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Given all this, the following is an example of the role I used where the variable CONTROLLER_CLUSTER_ROLE is set to the gitops-controller ClusterRole.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" name: gitops-controller aggregationRule: clusterRoleSelectors: - matchLabels: gitops/aggregate-to-controller: "true" rules: [] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: gitops-controller-admin labels: gitops/aggregate-to-controller: "true" aggregationRule: clusterRoleSelectors: - matchLabels: rbac.authorization.k8s.io/aggregate-to-admin: "true" rules: [] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: gitops-controller-view labels: gitops/aggregate-to-controller: "true" rules: - apiGroups: - '*' resources: - '*' verbs: - get - list - watch &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3. DISABLE_DEFAULT_ARGOCD_CONSOLELINK&lt;/h3&gt; &lt;p&gt;By default, the operator will create a default instance in the openshift-gitops namespace and create a &lt;a href="https://docs.openshift.com/container-platform/4.9/web_console/customizing-the-web-console.html#creating-custom-links_customizing-web-console"&gt;Console Link&lt;/a&gt; to access this instance. This custom link is rendered in the OpenShift console under the &lt;strong&gt;Applications&lt;/strong&gt; menu, visible to all OpenShift users, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift-console-link.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift-console-link.jpg?itok=l15iKZNu" width="384" height="478" alt="A screenshot of the console link in the OpenSift applications menu." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1. The console link in the OpenSift applications menu.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;However, this instance is typically used for cluster configuration, and we do not want tenants on the cluster to access this instance. While RBAC controls access to the instance, showing a link to users who cannot access it can be confusing.&lt;/p&gt; &lt;p&gt;Setting this variable to true will remove the ConsoleLink, and it will no longer appear in the console.&lt;/p&gt; &lt;h3&gt;4. DISABLE_DEFAULT_ARGOCD_INSTANCE&lt;/h3&gt; &lt;p&gt;As mentioned, the operator will create a cluster-scoped instance in the openshift-gitops namespace by default. Setting this environment variable to true will disable this behavior, and the default instance will not be created.&lt;/p&gt; &lt;p&gt;This can be useful when users do not require this instance (cluster configuration is managed remotely) or simply require a different namespace. An example of the latter could be users migrating from community Argo CD where the typical practice is to deploy in an argocd namespace.&lt;/p&gt; &lt;h3&gt;5. SERVER_CLUSTER_ROLE&lt;/h3&gt; &lt;p&gt;This environment variable is the equivalent of the previously discussed CONTROLLER_CLUSTER_ROLE except for the server component of Argo CD. It operates identically to that variable but binds the server service component to the specified ClusterRole.&lt;/p&gt; &lt;p&gt;In practice, there is typically much less need to customize or change this role than for the controller, but the option to do so is there.&lt;/p&gt; &lt;h2&gt;Customizing with environment variables&lt;/h2&gt; &lt;p&gt;This article described how to easily customize the behavior of the OpenShift GitOps operator using five environment variables in the &lt;strong&gt;Subscription&lt;/strong&gt; object. If you have questions, feel free to comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/06/5-global-environment-variables-provided-openshift-gitops" title="5 global environment variables provided by OpenShift GitOps"&gt;5 global environment variables provided by OpenShift GitOps&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Gerald Nunn</dc:creator><dc:date>2023-03-06T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.16.4.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-4-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-4-final-released/</id><updated>2023-03-06T00:00:00Z</updated><published>2023-03-06T00:00:00Z</published><summary type="html">While we are working hard on finalizing our 3.0 release, we are still baking maintenance releases for 2.16, and, today, here comes Quarkus 2.16.4.Final, our fourth maintenance release for the 2.16 release train. As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-03-06T00:00:00Z</dc:date></entry><entry><title>How RHEL image builder has improved security and function</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/02/how-rhel-image-builder-has-improved-security-and-function" /><author><name>Jacob Kozol</name></author><id>24713611-1dc3-4e3e-9e44-4540204c34cd</id><updated>2023-03-02T07:00:00Z</updated><published>2023-03-02T07:00:00Z</published><summary type="html">&lt;p&gt;Starting with Red Hat Enterprise Linux 8.7 and 9.1, image builder offers an easy way to embed containers. RHEL image builder works at build time and allows for containerized applications or services to be baked in, making the resulting images near production-ready. Furthermore, the image builder team worked with the OpenSCAP team to deliver security-hardened images. The OpenSCAP project provides security guidelines and baselines for securing images. This integration uses existing OpenSCAP tooling to run security remediation during the image build process.&lt;/p&gt; &lt;h2&gt;How to embed the container&lt;/h2&gt; &lt;p&gt;Containers can now be embedded into images at build time. The source of the container can be specified in the blueprint with the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[[containers]] source = "registry.access.redhat.com/ubi9:latest" name = “rhel-9” tls-verify = true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will download the container referred to by the source and add it to the image’s container registry. Of course you can use your own custom-built container image that provides the application or service you want to deploy.&lt;/p&gt; &lt;p&gt;If the registry you are pulling from requires authentication, the credentials can be provided by configuring the worker to read them from a containers-auth file. Create a containers-auth.json file and add the following to /etc/osbuild-worker/osbuild-worker.toml:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[containers] auth_file_path = "/etc/osbuild-worker/containers-auth.json" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The containers-auth.json file can be conveniently generated with Podman using the following command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;podman login --authfile=/etc/osbuild-worker/container-auth.json REGISTRY &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.osbuild.org/guides/blueprint-reference/blueprint-reference.html#containers"&gt;Refer to our guides&lt;/a&gt; for more information.&lt;/p&gt; &lt;h2&gt;Use OpenSCAP remediation at build time&lt;/h2&gt; &lt;p&gt;The reason for remediating the image during build time is to streamline the process by producing images that are security hardened and compliant out of the box.&lt;/p&gt; &lt;p&gt;Once an image has been built, it is difficult to ensure that it is security compliant. For example, the volumes would need to be repartitioned to verify that certain mount points are mounted to separate partitions. But if we remediate during the image build, we will already know that the partitions meet the criteria for a security benchmark.&lt;/p&gt; &lt;h3&gt;How to use the oscap tool&lt;/h3&gt; &lt;p&gt;OpenSCAP remediations can be enabled with customization options in the image builder &lt;a href="https://www.osbuild.org/guides/blueprint-reference/blueprint-reference.html#openscap-support"&gt;blueprint&lt;/a&gt;. To get started, add customizations to the blueprint. The &lt;a href="https://github.com/OpenSCAP/openscap"&gt;oscap command-line tool&lt;/a&gt; can assist in generating the following customizations:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[customizations.openscap] profile_id = "xccdf_org.ssgproject.content_profile_cis" datastream = "/usr/share/xml/scap/ssg/content/ssg-rhel8-ds.xml"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the customization, the profile is the CIS security benchmark. The data stream is the file that contains all the security rules and the remediation instructions.&lt;/p&gt; &lt;h2&gt;Wrap up&lt;/h2&gt; &lt;p&gt;We explained how image builder has improved and demonstrated how to remediate a RHEL 8.7 image. &lt;a href="https://www.osbuild.org/guides/user-guide/oscap-remediation.html"&gt;Refer to our guides&lt;/a&gt; for supported distros and security profiles. Once the image has been created, you can boot it up. If desired, you can scan it again using the &lt;a href="https://github.com/OpenSCAP/openscap/blob/maint-1.3/docs/manual/manual.adoc#generating-html-reports"&gt;oscap tool&lt;/a&gt; to see the results or generate a report for profile compliance.&lt;/p&gt; &lt;p&gt;Feel free to comment below if you have questions. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/02/how-rhel-image-builder-has-improved-security-and-function" title="How RHEL image builder has improved security and function"&gt;How RHEL image builder has improved security and function&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jacob Kozol</dc:creator><dc:date>2023-03-02T07:00:00Z</dc:date></entry><entry><title>What is Podman Desktop? A developer's introduction</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction" /><author><name>Ian Lawson</name></author><id>5296cf3a-de69-4480-a1b1-f9997bad92b2</id><updated>2023-03-01T07:00:00Z</updated><published>2023-03-01T07:00:00Z</published><summary type="html">&lt;p&gt;I'm a developer. Well, I like to think that I am; I spent twenty-odd years as a software engineer before joining Red Hat ten years ago, and since then, I've been evangelizing the company's tools and products from a developer perspective. I've seen the agile revolution and the rise of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and I was there when &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; crawled out of the sea and into our hearts.&lt;/p&gt; &lt;h2&gt;Developing locally in the container era&lt;/h2&gt; &lt;p&gt;But for the last four or so years, I've found developing containers locally a bit of a challenge. I'm used to being able to just log onto an &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; cluster and do my builds, normally through Source-2-Image or just by pointing the system at a Git repo with a Containerfile. But this isn't an option for a lot of developers.&lt;/p&gt; &lt;p&gt;I also used Docker a lot in the early days, but had a problem when I switched to developing on my Mac instead. To get around that problem, I actually hosted a Fedora virtual machine (VM), amusingly named 'builder', on which I did all my Docker builds. I would prepare all my source, create a Git repo, fire up the VM, ssh into it, clone the repo, build, test. Any problem I had, I would have to drop back to my Mac desktop, fix the code, git add, git commit, rinse and repeat.&lt;/p&gt; &lt;h2&gt;Podman benefits for developers&lt;/h2&gt; &lt;p&gt;Then along came Podman. Podman is, put simply, Docker with some security enhancements—it solves the old problem of having to run your containers as root, which was always a worry for me when using Docker. The &lt;a href="http://podman.io"&gt;Podman project&lt;/a&gt; actually defines Podman as "a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode."&lt;/p&gt; &lt;p&gt;As I mentioned, I use a Mac for development, and fortunately, there's a spin of Podman for the Mac. In fact, it's actually a pre-configured Fedora VM that is executed on the Mac, with appropriate privileges to connect to the host file system.&lt;/p&gt; &lt;p&gt;Podman gives me all the functionality I need to build, pull, push, and test containers. It is a command-line utility; some people prefer to use those rather than UX-based systems. I sit in both camps; I like to drop to the CLI when doing operations that need it, but I also like a nice, opinionated, rich UX. And that's where Podman Desktop comes in (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Desktop user interface." data-entity-type="file" data-entity-uuid="4667ac7d-0e0e-473e-90cf-762eafbb5a12" src="https://developers.redhat.com/sites/default/files/inline-images/podman1.jpg" width="2324" height="1424" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Podman Desktop user interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The &lt;a href="http://podman-desktop.io"&gt;website&lt;/a&gt; describes Podman Desktop as "an open source graphical tool enabling you to work with containers and Kubernetes from your local environment seamlessly."&lt;/p&gt; &lt;p&gt;These definitions of Podman and Podman Desktop do not do either of the projects justice. Being able to (natively) build and run containers on my MacBook is a huge advantage for me when it comes to building demos for Kubernetes and Red Hat OpenShift. I don't need to have a cluster up and running anywhere, and, in certain scenarios, I don't need internet connectivity either.&lt;/p&gt; &lt;p&gt;So, let's get started on a quick developer's introduction to using Podman Desktop.&lt;/p&gt; &lt;h2&gt;Installation and setup&lt;/h2&gt; &lt;p&gt;First, you need to get Podman installed on your machine. There are two distinct ways of doing this: If you go to &lt;a href="https://podman.io/getting-started/installation"&gt;https://podman.io/getting-started/installation&lt;/a&gt; and follow the instructions depending on your machine. As mentioned, I have a Mac; I originally installed Podman manually and then switched to Homebrew. Or, nicely, if you install Podman Desktop and Podman is not present, it offers to install it for you.&lt;/p&gt; &lt;p&gt;Once you have installed Podman, if you are using a Mac, you have to initialize and start the Podman engine (a step that isn't necessary on a native Fedora/&lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS&lt;/a&gt;/&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; installation). When using Podman and Podman Desktop, you might get occasional issues with the VM; you can fix these by restarting the Podman engine.&lt;/p&gt; &lt;p&gt;In an initial installation case, you enter &lt;code&gt;podman machine init&lt;/code&gt; and &lt;code&gt;podman machine start&lt;/code&gt;. Once the machine has started, do a &lt;code&gt;podman version&lt;/code&gt; to check the client and server are running at the same version (as shown in Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The Podman version in the command-line interface." data-entity-type="file" data-entity-uuid="a1572f8c-9779-42cb-a34a-773ea28ccd7a" src="https://developers.redhat.com/sites/default/files/inline-images/podman2.jpg" width="1265" height="925" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Checking the Podman version in the command-line interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now head to &lt;a href="https://podman-desktop.io/docs/Installation"&gt;https://podman-desktop.io/docs/Installation&lt;/a&gt; and follow the instructions for installing Podman Desktop to your appropriate operating system (OS). Once the installation has finished, start the application.&lt;/p&gt; &lt;h2&gt;Let’s talk registries&lt;/h2&gt; &lt;p&gt;Let's go over the absolute basics before we get onto registries. A container is actually a physical instance of an image. An image is a set of file-layers that are applied, in sequence, to the target file system of a container runtime host to instantiate a container. The concept of the file-layers in images is where it gets interesting.&lt;/p&gt; &lt;p&gt;Real-world example: Let's say I build an image that is made up of the base image of Fedora 37. I add the httpd service (using &lt;code&gt;dnf install httpd -y&lt;/code&gt;). I copy the content of my &lt;code&gt;web-app&lt;/code&gt; into the appropriate directory for the httpd. I add a &lt;code&gt;CMD ["httpd", "-DFOREGROUND]&lt;/code&gt; to get the container to execute the httpd service. This is all done using a Containerfile (which is the agnostic way of saying Dockerfile). When the build is created, it actually creates a set of file layers that are combined into an image.&lt;/p&gt; &lt;p&gt;It's one of those fun things to say: Images don't actually exist, they are a temporal map of file-layers. A registry contains a set of the "images," which are metadata maps of the &lt;em&gt;versioned&lt;/em&gt; file-layers attached to a tag (often &lt;code&gt;:latest&lt;/code&gt;, but this is bad practice as you lose all previous maps).&lt;/p&gt; &lt;p&gt;When you pull an image from a registry, what you are actually doing is pulling each of the file-layers that are listed as a map (the "image"). This makes images and containers very efficient in the way they are stored—for instance, if you have two images built from the same base image (set of file-layers), those shared file-layers exist just once in the registry.&lt;/p&gt; &lt;h3&gt;Registries in Podman and Podman Desktop&lt;/h3&gt; &lt;p&gt;From a Podman and Podman Desktop perspective, you can pull and push to registries, both local and remote. Registries are referred to using a Uniform Resource Identifier (URI). For example, the official Red Hat registry is located at registry.redhat.io. Images are referenced using the format &lt;code&gt;(registry)/(...optional subdirectories)/repo:tag&lt;/code&gt;; for instance, &lt;code&gt;quay.io/ilawson/devex4:latest&lt;/code&gt; refers to an image called &lt;code&gt;devex4&lt;/code&gt; with the tag &lt;code&gt;latest&lt;/code&gt; in the &lt;code&gt;ilawson&lt;/code&gt; repo at the quay.io registry.&lt;/p&gt; &lt;p&gt;In Podman, you work locally, pulling images from repos and pushing Images to repos. Any image you pull is written, using the file-layers, into your local pool, and when you push, the file-layers are transferred from your local pool to the target repo/image/tag.&lt;/p&gt; &lt;p&gt;Out in the real world, most of the registries have security—this is essential, to be honest, and when interacting with them from Podman, you need to have logged on to the registry. In the case of most of the popular registries, it is useful to create a "robot" at the registry for authenticated interactions; &lt;a href="https://access.redhat.com/RegistryAuthentication#creating-registry-service-accounts-6"&gt;there's a great article on how to create one of these&lt;/a&gt;, a "service account", for registry.redhat.io.&lt;/p&gt; &lt;p&gt;When using Podman Desktop, it interacts with the underlying Podman—Podman Desktop is a very good user experience that uses the Podman API. When using Podman Desktop to pull images from a registry that requires authentication, you can either pre-empt the calls by logging on to the registry through the Podman CLI, or usefully the Podman Desktop allows you to setup registry connections.&lt;/p&gt; &lt;p&gt;In Figure 3, you can see that I have logged Podman onto quay.io using my username and password, onto registry.redhat.io using a service account, and even into an active OpenShift cluster's internal registry using a username and a generated API token. By clicking on &lt;strong&gt;Add registry&lt;/strong&gt;, I can add others. Any interaction you then make to those registries (which are keyed by the URI discussed earlier) then use the authentication setup within Podman.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Registries list" data-entity-type="file" data-entity-uuid="0ffbc6c1-8a94-40e1-8bb4-f0aa37dd0dd2" src="https://developers.redhat.com/sites/default/files/inline-images/podman3.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Registries listed in the Podman UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Building images locally&lt;/h2&gt; &lt;p&gt;As I said, in the old days, I had to dump my source—a Dockerfile plus the local content—onto a Git repo, push it, log into my VM, and build it. Now, even on my Mac, I can run a "local" build directly.&lt;/p&gt; &lt;p&gt;In the following example, I have a directory on my machine that contains a Containerfile with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;FROM registry.fedoraproject.org/fedora:37 RUN dnf install -y httpd COPY content/* /var/www/html/ EXPOSE 80 CMD ["httpd", "-DFOREGROUND"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's a simple image containing the base fedora image at version 37, the httpd engine, and some source locally copied from the subdirectory content into the target directory &lt;code&gt;/var/www/html&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Using Podman Desktop, I can build this image directly into my local storage. In this case, I am going to name it appropriately to target an existing repo on quay.io; I won't build on or to quay.io, but the naming convention means I can build it locally and then push it directly to my repo when I need to.&lt;/p&gt; &lt;p&gt;By going to the &lt;strong&gt;Images&lt;/strong&gt; tab in Podman Desktop and then choosing &lt;strong&gt;Build Image&lt;/strong&gt;, I get the dialog shown in Figure 4:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Build dialog" data-entity-type="file" data-entity-uuid="06bf72fe-e0c8-4d04-9dcb-ce507bf89bde" src="https://developers.redhat.com/sites/default/files/inline-images/podman4.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Podman Build dialog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I provide a path to the Containerfile, as shown. I also provide a context directory; this allows the build to find and process the &lt;code&gt;content&lt;/code&gt; directory mentioned in the Containerfile. And I add an image name to store the composite file-layers upon build.&lt;/p&gt; &lt;p&gt;When the build completes, I now have an image in my local storage that I can test.&lt;/p&gt; &lt;p&gt;At the top right of the panel are several useful icons that let me update the image to a registry, run the image locally as a container, delete the image, and inspect the file-layers/operations executed to build the image.&lt;/p&gt; &lt;p&gt;By clicking on the run icon, I am presented with a full dialog for executing the image as a container, as shown in Figure 5. The basic dialog allows me to set the container name, map host volumes into the container, map a local port on my machine against the exposed port in the container. (In this case, run the container with access through port 9000 on the local machine into port 80 within the container, and specify some environment variables to be expressed into the container, which is a great way to inject configuration information for your application to process.)&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman create dialog" data-entity-type="file" data-entity-uuid="89c6ed78-8b2f-4a10-ac1c-9adef9645b5c" src="https://developers.redhat.com/sites/default/files/inline-images/podman5.jpg" width="1118" height="668" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: The create dialog in Podman.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once running, I have a superb overview of the active container (plus any others I have running). This gives me direct access to the state of the container and the ability to generate Kube object definitions, deploy it to Kubernetes (more in a sec), open the port in a browser, open a terminal directly into the container and force a restart.&lt;/p&gt; &lt;p&gt;All of these are available via the command line, but having them easily reachable in a graphical fashion makes interacting with the containers so much easier. If I open a terminal from the UI, I can interact directly with the container and its file-system (Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman terminal dialog" data-entity-type="file" data-entity-uuid="c2cb6a4d-85f3-4fdc-980a-f8fa42fa3c19" src="https://developers.redhat.com/sites/default/files/inline-images/podman6.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Podman terminal dialog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is the source of the web content I wrote into the image using the Containerfile. As you can see from the left-hand panel, Podman Desktop gives a real-time overview of the Images and containers currently active.&lt;/p&gt; &lt;p&gt;If you now choose &lt;strong&gt;Images&lt;/strong&gt; on the left-hand side of the application, you will get a full list of the Images currently accessible locally on your machine.&lt;/p&gt; &lt;h3&gt;Using the OpenShift extension in Podman&lt;/h3&gt; &lt;p&gt;What is nice about this is that, as stated, all of this is currently local to my machine. As part of the test, I added the OpenShift extension to Podman Desktop, which allows me to interact directly with an OpenShift cluster. &lt;/p&gt; &lt;p&gt;The OpenShift extension uses my current Kube config as a route into OpenShift. Having logged on locally (via &lt;code&gt;oc&lt;/code&gt;) I can now see and interact with any of the projects I have rights to on the target cluster. In addition, I can push any of my local images as deployments to the cluster, meaning I can develop and test an image/container locally, then upload and test on my target OpenShift. By default, Podman Desktop also provides connectivity to &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt;, a locally instantiated small OpenShift instance. OpenShift Local used to be called Red Hat CodeReady Containers, and, as of January 2023, Podman Desktop still refers to it as such.&lt;/p&gt; &lt;p&gt;The other really nice feature of the OpenShift extension is that I can choose an Image and not only deploy it to the target cluster, but also either push to a remote registry and then deploy or push to the integrated registry in OpenShift, assuming I have access, and then deploy. This is shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman deploy to OpenShift dialog" data-entity-type="file" data-entity-uuid="a1c7adfe-9783-4f5d-baea-572b185c0ef8" src="https://developers.redhat.com/sites/default/files/inline-images/podman7.jpg" width="1674" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: Deploying to OpenShift in Podman.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;From a developer perspective, Podman Desktop really eases the pain of building images and hosting containers locally. The product is constantly evolving, with some great features around more OpenShift integration coming soon, and the fact it now runs happily on my Mac makes me a happy developer for a change.&lt;/p&gt; &lt;p class="Indent1"&gt;New to working with Podman? Download our &lt;a href="https://developers.redhat.com/cheat-sheets/podman-cheat-sheet"&gt;Podman Cheat Sheet&lt;/a&gt;, which covers basic commands for managing images, containers, and container resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction" title="What is Podman Desktop? A developer's introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ian Lawson</dc:creator><dc:date>2023-03-01T07:00:00Z</dc:date></entry><entry><title>How to use continuous integration with Jenkins on OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/28/how-use-continuous-integration-jenkins-openshift" /><author><name>Nagesh Rathod</name></author><id>4d9a8b03-4e1a-4905-a9cc-72150fec88eb</id><updated>2023-02-28T07:00:00Z</updated><published>2023-02-28T07:00:00Z</published><summary type="html">&lt;p&gt;In this article series, we will set up a CI pipeline to compile and package a JavaScript game application into a Docker image using Jenkins on Red Hat OpenShift. Once we build the image, it will be pushed to the external Red Hat Quay container registry, &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt;. When the developer pushes the changes into the Git repository, all these actions trigger.&lt;/p&gt; &lt;p&gt;This is a series of complete CI/CD pipelines on OpenShift using the Jenkins and Red Hat Ansible Automation Platform. We will cover the following topics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Part 1: Continuous integration with Jenkins on OpenShift&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-employ-continuous-deployment-ansible-openshift"&gt;Continuous deployment using Ansible Automation Platform on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-manual-intervention-pipeline-restricts-deployment"&gt;How a manual intervention pipeline restricts deployment&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This article is based on the assumption that you have basic knowledge of Jenkins, OpenShift, and Ansible Automation Platform. You will also need administrator privileges for your Openshift cluster.&lt;/p&gt; &lt;h2&gt;The CI pipeline architecture&lt;/h2&gt; &lt;p&gt;The developer commits and pushes the changes after initiating the action, as shown in the architecture diagram (Figure 1). Jenkins will detect the changes with the help of polling or webhooks. We build the image in the OpenShift cluster and push it to the Quay.io container registry using buildconfig.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_0.jpg?itok=soqNMQ_1" width="600" height="275" alt="A diagram of continuous integration architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: A continuous integration architecture diagram.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Install Jenkins on OpenShift&lt;/h2&gt; &lt;p&gt;Now we need Jenkins Dashboard, which will run the CI pipeline. The easiest way is to deploy a pod of Jenkins on OpenShift from the developer's catalog, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2.jpg?itok=iU5A8rKt" width="600" height="292" alt="A screenshot of the Developer Catalog from where you can install Jenkins." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Install Jenkins from the Developer Catalog.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Follow these six steps to install Jenkins on OpenShift:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;From OpenShift Web Console, switch to the &lt;strong&gt;Developer&lt;/strong&gt; perspective and navigate to the &lt;strong&gt;Topology&lt;/strong&gt; view. Click on +Add &gt; From Developer Catalog &gt; All services&lt;/li&gt; &lt;li&gt;Search for Jenkins.&lt;/li&gt; &lt;li&gt;Select the persistence Jenkins and install it (For this article, I am keeping the settings as default, but you can modify the settings as per your requirement).&lt;/li&gt; &lt;li&gt;After installation, one Jenkins pod should appear in the project.&lt;/li&gt; &lt;li&gt;To access the dashboard of Jenkins, click on the route icon.&lt;/li&gt; &lt;li&gt;For Jenkins dashboard access, you can use the OpenShift console credential, as shown in Figure 3.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3.jpg?itok=r5VKrDaY" width="600" height="362" alt="The Jenkins login screen with OpenShift credentials." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The Jenkins login screen with OpenShift credentials.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Set up Jenkins CI pipeline&lt;/h2&gt; &lt;p&gt;The continuous integration stage consists of building and pushing the image into the container registry.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Make sure to have one git repository in place, including the Dockerfile and application dependencies like the requirements.txt file.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a Jenkinsfile with the following contents and add this jenkinsfile to the git repository.&lt;/li&gt; &lt;/ol&gt;&lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pipeline {     agent any     stages {         stage('Hello') {             steps {                 echo 'Hello World'             }         }         stage("Checkout") {             steps {                 checkout scm             }         }         stage("Docker Build") {             steps {               sh '''                   #oc start-build --from-build=&lt;build_name&gt;                   oc start-build -F red-api --from-dir=./api/               '''             }         }     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt; &lt;p class="Indent1"&gt;3. Next, create a BuildConfig file using the following content that will build the source code to executable and push.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: build.OpenShift.io/v1 kind: BuildConfig metadata:   labels:     app.kubernetes.io/name: red-api # your application name   name: red-api # your application name spec:   output:     to:       kind: DockerImage       name: ***************** # add yourimage   source:     # Expect a local directory to be streamed to OpenShift as a build source     type: Binary     binary: {}   strategy:     type: Docker     dockerStrategy:       # Find the image build instructions in./Dockerfile       dockerfilePath: Dockerfile&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Ex: name: quay.io/&lt;username&gt;/cd:latest&lt;/p&gt; &lt;p class="Indent1"&gt;4. Next, create the secrets which will help our build config to push our recently built image in the container registry.&lt;/p&gt; &lt;p class="Indent1"&gt;5. To create a secret, type the following command into your terminal and make sure to use the username and password of your environment. For this exercise, we are using a &lt;a href="https://quay.io/"&gt;&lt;u&gt;Quay.io&lt;/u&gt;&lt;/a&gt; container registry.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret docker-registry my-secret --docker-server=quay.io --docker-username=xxxx  --docker-password=xxx &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc secrets link builder my-secret --for=mount&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;6. Next, we will create a pipeline from the Jenkins dashboard (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4.jpg?itok=h6_F-p21" width="600" height="282" alt="A screenshot of the Jenkins dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The Jenkins dashboard​​​​​ after installation.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;7. Select &lt;strong&gt;Pipeline&lt;/strong&gt; from &lt;strong&gt;New Item&lt;/strong&gt; and give a name to that pipeline.&lt;/p&gt; &lt;p class="Indent1"&gt;8. Select the &lt;strong&gt;Build Triggers&lt;/strong&gt; when the changes are pushed in the GitHub repository.&lt;/p&gt; &lt;p class="Indent1"&gt;9. In the pipeline, select &lt;strong&gt;Pipeline Script&lt;/strong&gt; from &lt;a href="https://github.com/redhat-developer-demos/ansible-automation-platform-continous-delivery-demo"&gt;&lt;u&gt;SCM&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;10. Fill in the details according to the snapshot shown in Figure 5 for the config we're using for this article.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_3.png?itok=q_BJHr_I" width="600" height="284" alt="Ansible Automation Platform integrate with jenkins pipeline​​​​​​." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Integrate Ansible Automation Platform with Jenkins pipeline​​​​​​.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;11. Once the pipeline is ready, execute it by clicking the &lt;strong&gt;Build Now&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;button. You can see a glimpse of the pipeline in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/6.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/6.jpg?itok=Jt15EUxw" width="600" height="284" alt="A screenshot showing the completed CI stage and the CI pipeline running in the Jenkins dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: The CI pipeline running in the Jenkins dashboard.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;What’s Next?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-employ-continuous-deployment-ansible-openshift"&gt;next article&lt;/a&gt; is based on the continuous deployment using the Ansible Automation platform on the OpenShift cluster. You will learn how to install the Ansible Automation platform using the operator’s hub on OpenShift and also the integration of Jenkins and Ansible Automation platform. Check out a &lt;a href="https://developers.redhat.com/devnation/devnationday-india-2022/ci-cd-ansible-automation-platform"&gt;demo&lt;/a&gt; of this project in DevNation2022.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-use-continuous-integration-jenkins-openshift" title="How to use continuous integration with Jenkins on OpenShift"&gt;How to use continuous integration with Jenkins on OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-02-28T07:00:00Z</dc:date></entry><entry><title>Edge computing: From 30 tons to 30 grams</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/27/edge-computing-101" /><author><name>Don Schenck</name></author><id>72574f1e-f86f-458b-b8df-38c160d05879</id><updated>2023-02-27T07:00:00Z</updated><published>2023-02-27T07:00:00Z</published><summary type="html">&lt;p&gt;When the ENIAC computer was introduced in 1946, it was housed in a huge room—1,800 square feet—and weighed 30 tons. It had to be assembled in place, and it wasn't going to be moved. The era of electronic computers had arrived, but only for an elite few. The idea of &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt; was science fiction—unbelievable science fiction at that. My, how things have changed.&lt;/p&gt; &lt;h2&gt;Mainframes&lt;/h2&gt; &lt;p&gt;The IBM mainframe computers, introduced in 1952, became the standard of computing for corporations and government agencies in the 1960s and 1970s. Those of us old enough can remember, for example, getting their home water bill in the form of a punched card with the words "&lt;a href="https://en.wikipedia.org/wiki/Punched_card#Do_Not_Fold,_Spindle_or_Mutilate"&gt;Do not fold, spindle or mutilate&lt;/a&gt;" on it. These mainframe computers moved processing to the corporate headquarters. Sales from cash registers, for example, would be sent to headquarters on punched paper tape where it could be read into the mainframes for reporting.&lt;/p&gt; &lt;p&gt;(Author's note: My first job in IT was processing paper tape into a mainframe.)&lt;/p&gt; &lt;h2&gt;Midrange computers&lt;/h2&gt; &lt;p&gt;In the 1970s, minicomputers (also called midrange computers) became very popular. The Digital VAX, Data General Nova, and the hugely popular IBM System/3x-400 series (System/3, System/32, System/34/, System/36, System/38, and AS/400) moved computing power even closer to the action. Midrange systems started in air-conditioned rooms with raised flooring, then moved to corners in offices, then eventually under desks as they grew in power and shrunk in physical size. Remote offices and small businesses now had a computer in their building. Larger midrange computers started to move into the spaces formerly occupied by the mainframes.&lt;/p&gt; &lt;h2&gt;The PC&lt;/h2&gt; &lt;p&gt;The 1980s saw the dawn of the desktop PC, and the trend of computing moving closer and closer to the action continued. A small desktop PC, for example, could be integrated into a production environment on a factory floor to record data and control machines. The PC would, typically, send the data to a host—often a midrange computer—and, likewise, get data from the host. This happened over a network: Twinax or ethernet cabling and the associated protocol—SNA, Novell Netware, and ethernet were the choices.&lt;/p&gt; &lt;h2&gt;Portables&lt;/h2&gt; &lt;p&gt;Adam Osborne brought the popular portable PC to the world in 1981 with the 24.5-pound Osborne 1. While more "luggable" than portable, this enabled the computer to move around more easily. Again, the processing power was moving closer to the action.&lt;/p&gt; &lt;p&gt;Laptops and notebook computers followed and continue to evolve to this day, with a WiFi connection now being a requirement.&lt;/p&gt; &lt;h2&gt;Tablets and phones&lt;/h2&gt; &lt;p&gt;In the early 1990s, the personal digital assistant, or PDA, arrived. The star was the Palm Pilot, a small device that could store and record information. Users could take notes, make voice recordings, or manage their schedules. This machine was synched to a PC via a cable. This meant, while not real-time, processing could now be carried around in a pocket.&lt;/p&gt; &lt;p&gt;The PDA was the genesis of and gave way to today's "must-have" item, the smartphone. Using WiFi and wireless 5G technology, the smartphone enables real-time data processing in a small form factor. This is one example of computing at the edge.&lt;/p&gt; &lt;p&gt;Likewise, small tablets such as the iPad allow users to carry processing power with them, along with ample screen real estate and advanced communication functionality.&lt;/p&gt; &lt;h2&gt;Embedded systems and closer to the edge&lt;/h2&gt; &lt;p&gt;But it goes further, and deeper.&lt;/p&gt; &lt;p&gt;Very small sensors and controllers can now be embedded into everyday items. These &lt;a href="https://developers.redhat.com/topics/iot"&gt;Internet of Things (IoT)&lt;/a&gt; devices range from thermostats to watches—the Apple watch weighs in at just over 30 grams, hence the title of this article—to, well, almost everything. You can even buy a ring that senses and reports data.&lt;/p&gt; &lt;p&gt;These IoT devices might (or might not) have some computing ability beyond just collecting data, but they do often communicate with an edge computer or a cloud-based system that has advanced capabilities. Processing that occurs at or near the edge is where the greatest challenges are presented. Things such as enforcing security and installing updates are paramount because this is where the action takes place.&lt;/p&gt; &lt;p&gt;All of this makes up "the edge." Sensing and processing move closer and closer to where the events occur. Communication capabilities have also improved, with WiFi, 5G, NFC, and more making it easier and more likely that edge devices will communicate with &lt;em&gt;each other&lt;/em&gt;. An in-car network, for example, can improve automotive travel; &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-and-general-motors-collaborate-trailblaze-future-software-defined-vehicles"&gt;Red Hat is working with GM on that very technology&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Edge computing example: The modern automobile&lt;/h2&gt; &lt;p&gt;Let's consider the advanced automobile as a use case for edge computing.&lt;/p&gt; &lt;p&gt;Sensors are used to report current speed, location, road conditions, outside temperature, lane edges, surrounding vehicles, and much more. These readings are reported to the driver and the drivetrain. The driver can use the information to make decisions, while an onboard computer can use the data to make adjustments—keep the car within the lanes, reduce speed based on front-facing radar, reduce torque based on road conditions, and much more.&lt;/p&gt; &lt;p&gt;They could be expanded when "smart highways" are introduced. Sensors can keep track of traffic density and speed; accidents can be used as data, reporting to cars to determine speeds and, perhaps, a new route based on congestion.&lt;/p&gt; &lt;p&gt;All this edge computing will need to be secure, and systems will need to be updated. We already have cars that can receive software updates while offline, i.e., not on the road. I can check the fuel level of my MINI from my smartphone.&lt;/p&gt; &lt;h2&gt;The future: Bright or dark?&lt;/h2&gt; &lt;p&gt;This is all just a start, only the beginning of more advanced systems running closer and closer to the action. One can easily wonder: Is Kurzweil's Singularity at hand?&lt;/p&gt; &lt;p&gt;The future, truly, is at the edge.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/27/edge-computing-101" title="Edge computing: From 30 tons to 30 grams"&gt;Edge computing: From 30 tons to 30 grams&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2023-02-27T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 14th September 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-02-24.html" /><category term="quarkus" /><category term="kogito" /><category term="vertx" /><category term="drools" /><category term="cloudevents" /><category term="hibernate" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-02-24.html</id><updated>2023-02-24T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kogito, vertx, drools, cloudevents, hibernate"&gt; &lt;h1&gt;This Week in JBoss - 14th September 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welocme back everyone! I don’t know about where you happen to be, but where I’m living, we had the second largest snow storm in recorded weather history. I’ve been digging out my driveway and front door for a couple of days. Needless to say, it has been a bit cold here.&lt;/p&gt; &lt;p&gt;Also, my thoughts go out to those affected by the war between Russia and Ukraine, on this one year anniversary. Please stay as safe as you can.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/kogito-1-33-0-released.html"&gt;Kogito 1.33.0&lt;/a&gt; - This release includes some version bumps, further integrations, and of course, some bug fixes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-3-8/"&gt;Eclipse Vert.x 4.3.8&lt;/a&gt; - Mostly a bug fix release, but if you’re running Vert.x on Windows, it contains a fix for &lt;a href="https://github.com/vert-x3/vertx-web/security/advisories/GHSA-53jx-vvf9-4x38"&gt;CVE-2023-24815&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-16-2-final-released/"&gt;Quarkus 2.16.2.Final&lt;/a&gt; - Some bugfixes and documentation improvements mostly.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html"&gt;Cloudevents Labeling and Classification with Drools&lt;/a&gt; - Matteo demos using declarative logic, persistence, eventing, and other tech to efficiently label and store Cloudevents.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/"&gt;DataCater uses Quarkus to make Data Streaming more accessible&lt;/a&gt; - Stefan explores DataCater and their reasoning in chosing to use Quarkus to replace their usage of Play! framework in reworking their data pipeline.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/jpastreamer-extension/"&gt;Express Hibernate Queries as Type-Safe Java Streams&lt;/a&gt; - Julia discusses the library &lt;code&gt;JPAStreamer.&lt;/code&gt; It allows you to write persistence queries in a more stream-like manner and also making the queries performant.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html"&gt;Serverless Workflow Integration with Camel Routes&lt;/a&gt; - Ricardo demonstrates the integration of Camel Routes with Kogito Serverless Workflow.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/newsletter/29/"&gt;Quarkus Newsletter #29&lt;/a&gt; - Monthly Quarkus Newsletter for more Quarkus related information&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/jboss-maven/apache-maven-faqs/"&gt;Apache Maven Faqs for Java Developers&lt;/a&gt; - If you’re new to Apache Maven, or simply have some questions, this quick FAQ may help you out.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=H9yK0xnExeA"&gt;Quarkus Insights 118&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_until_next_time"&gt;Until next time!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Again, everyone stay safe, and I hope that your tests stay green!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry></feed>
