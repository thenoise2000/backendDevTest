<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>What is Podman Desktop? A developer's introduction</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction" /><author><name>Ian Lawson</name></author><id>5296cf3a-de69-4480-a1b1-f9997bad92b2</id><updated>2023-03-01T07:00:00Z</updated><published>2023-03-01T07:00:00Z</published><summary type="html">&lt;p&gt;I'm a developer. Well, I like to think that I am; I spent twenty-odd years as a software engineer before joining Red Hat ten years ago, and since then, I've been evangelizing the company's tools and products from a developer perspective. I've seen the agile revolution and the rise of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and I was there when &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; crawled out of the sea and into our hearts.&lt;/p&gt; &lt;h2&gt;Developing locally in the container era&lt;/h2&gt; &lt;p&gt;But for the last four or so years, I've found developing containers locally a bit of a challenge. I'm used to being able to just log onto an &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; cluster and do my builds, normally through Source-2-Image or just by pointing the system at a Git repo with a Containerfile. But this isn't an option for a lot of developers.&lt;/p&gt; &lt;p&gt;I also used Docker a lot in the early days, but had a problem when I switched to developing on my Mac instead. To get around that problem, I actually hosted a Fedora virtual machine (VM), amusingly named 'builder', on which I did all my Docker builds. I would prepare all my source, create a Git repo, fire up the VM, ssh into it, clone the repo, build, test. Any problem I had, I would have to drop back to my Mac desktop, fix the code, git add, git commit, rinse and repeat.&lt;/p&gt; &lt;h2&gt;Podman benefits for developers&lt;/h2&gt; &lt;p&gt;Then along came Podman. Podman is, put simply, Docker with some security enhancements—it solves the old problem of having to run your containers as root, which was always a worry for me when using Docker. The &lt;a href="http://podman.io"&gt;Podman project&lt;/a&gt; actually defines Podman as "a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode."&lt;/p&gt; &lt;p&gt;As I mentioned, I use a Mac for development, and fortunately, there's a spin of Podman for the Mac. In fact, it's actually a pre-configured Fedora VM that is executed on the Mac, with appropriate privileges to connect to the host file system.&lt;/p&gt; &lt;p&gt;Podman gives me all the functionality I need to build, pull, push, and test containers. It is a command-line utility; some people prefer to use those rather than UX-based systems. I sit in both camps; I like to drop to the CLI when doing operations that need it, but I also like a nice, opinionated, rich UX. And that's where Podman Desktop comes in (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Desktop user interface." data-entity-type="file" data-entity-uuid="4667ac7d-0e0e-473e-90cf-762eafbb5a12" src="https://developers.redhat.com/sites/default/files/inline-images/podman1.jpg" width="2324" height="1424" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Podman Desktop user interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The &lt;a href="http://podman-desktop.io"&gt;website&lt;/a&gt; describes Podman Desktop as "an open source graphical tool enabling you to work with containers and Kubernetes from your local environment seamlessly."&lt;/p&gt; &lt;p&gt;These definitions of Podman and Podman Desktop do not do either of the projects justice. Being able to (natively) build and run containers on my MacBook is a huge advantage for me when it comes to building demos for Kubernetes and Red Hat OpenShift. I don't need to have a cluster up and running anywhere, and, in certain scenarios, I don't need internet connectivity either.&lt;/p&gt; &lt;p&gt;So, let's get started on a quick developer's introduction to using Podman Desktop.&lt;/p&gt; &lt;h2&gt;Installation and setup&lt;/h2&gt; &lt;p&gt;First, you need to get Podman installed on your machine. There are two distinct ways of doing this: If you go to &lt;a href="https://podman.io/getting-started/installation"&gt;https://podman.io/getting-started/installation&lt;/a&gt; and follow the instructions depending on your machine. As mentioned, I have a Mac; I originally installed Podman manually and then switched to Homebrew. Or, nicely, if you install Podman Desktop and Podman is not present, it offers to install it for you.&lt;/p&gt; &lt;p&gt;Once you have installed Podman, if you are using a Mac, you have to initialize and start the Podman engine (a step that isn't necessary on a native Fedora/&lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS&lt;/a&gt;/&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; installation). When using Podman and Podman Desktop, you might get occasional issues with the VM; you can fix these by restarting the Podman engine.&lt;/p&gt; &lt;p&gt;In an initial installation case, you enter &lt;code&gt;podman machine init&lt;/code&gt; and &lt;code&gt;podman machine start&lt;/code&gt;. Once the machine has started, do a &lt;code&gt;podman version&lt;/code&gt; to check the client and server are running at the same version (as shown in Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The Podman version in the command-line interface." data-entity-type="file" data-entity-uuid="a1572f8c-9779-42cb-a34a-773ea28ccd7a" src="https://developers.redhat.com/sites/default/files/inline-images/podman2.jpg" width="1265" height="925" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Checking the Podman version in the command-line interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now head to &lt;a href="https://podman-desktop.io/docs/Installation"&gt;https://podman-desktop.io/docs/Installation&lt;/a&gt; and follow the instructions for installing Podman Desktop to your appropriate operating system (OS). Once the installation has finished, start the application.&lt;/p&gt; &lt;h2&gt;Let’s talk registries&lt;/h2&gt; &lt;p&gt;Let's go over the absolute basics before we get onto registries. A container is actually a physical instance of an image. An image is a set of file-layers that are applied, in sequence, to the target file system of a container runtime host to instantiate a container. The concept of the file-layers in images is where it gets interesting.&lt;/p&gt; &lt;p&gt;Real-world example: Let's say I build an image that is made up of the base image of Fedora 37. I add the httpd service (using &lt;code&gt;dnf install httpd -y&lt;/code&gt;). I copy the content of my &lt;code&gt;web-app&lt;/code&gt; into the appropriate directory for the httpd. I add a &lt;code&gt;CMD ["httpd", "-DFOREGROUND]&lt;/code&gt; to get the container to execute the httpd service. This is all done using a Containerfile (which is the agnostic way of saying Dockerfile). When the build is created, it actually creates a set of file layers that are combined into an image.&lt;/p&gt; &lt;p&gt;It's one of those fun things to say: Images don't actually exist, they are a temporal map of file-layers. A registry contains a set of the "images," which are metadata maps of the &lt;em&gt;versioned&lt;/em&gt; file-layers attached to a tag (often &lt;code&gt;:latest&lt;/code&gt;, but this is bad practice as you lose all previous maps).&lt;/p&gt; &lt;p&gt;When you pull an image from a registry, what you are actually doing is pulling each of the file-layers that are listed as a map (the "image"). This makes images and containers very efficient in the way they are stored—for instance, if you have two images built from the same base image (set of file-layers), those shared file-layers exist just once in the registry.&lt;/p&gt; &lt;h3&gt;Registries in Podman and Podman Desktop&lt;/h3&gt; &lt;p&gt;From a Podman and Podman Desktop perspective, you can pull and push to registries, both local and remote. Registries are referred to using a Uniform Resource Identifier (URI). For example, the official Red Hat registry is located at registry.redhat.io. Images are referenced using the format &lt;code&gt;(registry)/(...optional subdirectories)/repo:tag&lt;/code&gt;; for instance, &lt;code&gt;quay.io/ilawson/devex4:latest&lt;/code&gt; refers to an image called &lt;code&gt;devex4&lt;/code&gt; with the tag &lt;code&gt;latest&lt;/code&gt; in the &lt;code&gt;ilawson&lt;/code&gt; repo at the quay.io registry.&lt;/p&gt; &lt;p&gt;In Podman, you work locally, pulling images from repos and pushing Images to repos. Any image you pull is written, using the file-layers, into your local pool, and when you push, the file-layers are transferred from your local pool to the target repo/image/tag.&lt;/p&gt; &lt;p&gt;Out in the real world, most of the registries have security—this is essential, to be honest, and when interacting with them from Podman, you need to have logged on to the registry. In the case of most of the popular registries, it is useful to create a "robot" at the registry for authenticated interactions; &lt;a href="https://access.redhat.com/RegistryAuthentication#creating-registry-service-accounts-6"&gt;there's a great article on how to create one of these&lt;/a&gt;, a "service account", for registry.redhat.io.&lt;/p&gt; &lt;p&gt;When using Podman Desktop, it interacts with the underlying Podman—Podman Desktop is a very good user experience that uses the Podman API. When using Podman Desktop to pull images from a registry that requires authentication, you can either pre-empt the calls by logging on to the registry through the Podman CLI, or usefully the Podman Desktop allows you to setup registry connections.&lt;/p&gt; &lt;p&gt;In Figure 3, you can see that I have logged Podman onto quay.io using my username and password, onto registry.redhat.io using a service account, and even into an active OpenShift cluster's internal registry using a username and a generated API token. By clicking on &lt;strong&gt;Add registry&lt;/strong&gt;, I can add others. Any interaction you then make to those registries (which are keyed by the URI discussed earlier) then use the authentication setup within Podman.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Registries list" data-entity-type="file" data-entity-uuid="0ffbc6c1-8a94-40e1-8bb4-f0aa37dd0dd2" src="https://developers.redhat.com/sites/default/files/inline-images/podman3.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Registries listed in the Podman UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Building images locally&lt;/h2&gt; &lt;p&gt;As I said, in the old days, I had to dump my source—a Dockerfile plus the local content—onto a Git repo, push it, log into my VM, and build it. Now, even on my Mac, I can run a "local" build directly.&lt;/p&gt; &lt;p&gt;In the following example, I have a directory on my machine that contains a Containerfile with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;FROM registry.fedoraproject.org/fedora:37 RUN dnf install -y httpd COPY content/* /var/www/html/ EXPOSE 80 CMD ["httpd", "-DFOREGROUND"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's a simple image containing the base fedora image at version 37, the httpd engine, and some source locally copied from the subdirectory content into the target directory &lt;code&gt;/var/www/html&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Using Podman Desktop, I can build this image directly into my local storage. In this case, I am going to name it appropriately to target an existing repo on quay.io; I won't build on or to quay.io, but the naming convention means I can build it locally and then push it directly to my repo when I need to.&lt;/p&gt; &lt;p&gt;By going to the &lt;strong&gt;Images&lt;/strong&gt; tab in Podman Desktop and then choosing &lt;strong&gt;Build Image&lt;/strong&gt;, I get the dialog shown in Figure 4:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman Build dialog" data-entity-type="file" data-entity-uuid="06bf72fe-e0c8-4d04-9dcb-ce507bf89bde" src="https://developers.redhat.com/sites/default/files/inline-images/podman4.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Podman Build dialog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I provide a path to the Containerfile, as shown. I also provide a context directory; this allows the build to find and process the &lt;code&gt;content&lt;/code&gt; directory mentioned in the Containerfile. And I add an image name to store the composite file-layers upon build.&lt;/p&gt; &lt;p&gt;When the build completes, I now have an image in my local storage that I can test.&lt;/p&gt; &lt;p&gt;At the top right of the panel are several useful icons that let me update the image to a registry, run the image locally as a container, delete the image, and inspect the file-layers/operations executed to build the image.&lt;/p&gt; &lt;p&gt;By clicking on the run icon, I am presented with a full dialog for executing the image as a container, as shown in Figure 5. The basic dialog allows me to set the container name, map host volumes into the container, map a local port on my machine against the exposed port in the container. (In this case, run the container with access through port 9000 on the local machine into port 80 within the container, and specify some environment variables to be expressed into the container, which is a great way to inject configuration information for your application to process.)&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman create dialog" data-entity-type="file" data-entity-uuid="89c6ed78-8b2f-4a10-ac1c-9adef9645b5c" src="https://developers.redhat.com/sites/default/files/inline-images/podman5.jpg" width="1118" height="668" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: The create dialog in Podman.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once running, I have a superb overview of the active container (plus any others I have running). This gives me direct access to the state of the container and the ability to generate Kube object definitions, deploy it to Kubernetes (more in a sec), open the port in a browser, open a terminal directly into the container and force a restart.&lt;/p&gt; &lt;p&gt;All of these are available via the command line, but having them easily reachable in a graphical fashion makes interacting with the containers so much easier. If I open a terminal from the UI, I can interact directly with the container and its file-system (Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman terminal dialog" data-entity-type="file" data-entity-uuid="c2cb6a4d-85f3-4fdc-980a-f8fa42fa3c19" src="https://developers.redhat.com/sites/default/files/inline-images/podman6.jpg" width="1162" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Podman terminal dialog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is the source of the web content I wrote into the image using the Containerfile. As you can see from the left-hand panel, Podman Desktop gives a real-time overview of the Images and containers currently active.&lt;/p&gt; &lt;p&gt;If you now choose &lt;strong&gt;Images&lt;/strong&gt; on the left-hand side of the application, you will get a full list of the Images currently accessible locally on your machine.&lt;/p&gt; &lt;h3&gt;Using the OpenShift extension in Podman&lt;/h3&gt; &lt;p&gt;What is nice about this is that, as stated, all of this is currently local to my machine. As part of the test, I added the OpenShift extension to Podman Desktop, which allows me to interact directly with an OpenShift cluster. &lt;/p&gt; &lt;p&gt;The OpenShift extension uses my current Kube config as a route into OpenShift. Having logged on locally (via &lt;code&gt;oc&lt;/code&gt;) I can now see and interact with any of the projects I have rights to on the target cluster. In addition, I can push any of my local images as deployments to the cluster, meaning I can develop and test an image/container locally, then upload and test on my target OpenShift. By default, Podman Desktop also provides connectivity to &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt;, a locally instantiated small OpenShift instance. OpenShift Local used to be called Red Hat CodeReady Containers, and, as of January 2023, Podman Desktop still refers to it as such.&lt;/p&gt; &lt;p&gt;The other really nice feature of the OpenShift extension is that I can choose an Image and not only deploy it to the target cluster, but also either push to a remote registry and then deploy or push to the integrated registry in OpenShift, assuming I have access, and then deploy. This is shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Podman deploy to OpenShift dialog" data-entity-type="file" data-entity-uuid="a1c7adfe-9783-4f5d-baea-572b185c0ef8" src="https://developers.redhat.com/sites/default/files/inline-images/podman7.jpg" width="1674" height="712" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: Deploying to OpenShift in Podman.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;From a developer perspective, Podman Desktop really eases the pain of building images and hosting containers locally. The product is constantly evolving, with some great features around more OpenShift integration coming soon, and the fact it now runs happily on my Mac makes me a happy developer for a change.&lt;/p&gt; &lt;p class="Indent1"&gt;New to working with Podman? Download our &lt;a href="https://developers.redhat.com/cheat-sheets/podman-cheat-sheet"&gt;Podman Cheat Sheet&lt;/a&gt;, which covers basic commands for managing images, containers, and container resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction" title="What is Podman Desktop? A developer's introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ian Lawson</dc:creator><dc:date>2023-03-01T07:00:00Z</dc:date></entry><entry><title>How to use continuous integration with Jenkins on OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/28/how-use-continuous-integration-jenkins-openshift" /><author><name>Nagesh Rathod</name></author><id>4d9a8b03-4e1a-4905-a9cc-72150fec88eb</id><updated>2023-02-28T07:00:00Z</updated><published>2023-02-28T07:00:00Z</published><summary type="html">&lt;p&gt;In this article series, we will set up a CI pipeline to compile and package a JavaScript game application into a Docker image using Jenkins on Red Hat OpenShift. Once we build the image, it will be pushed to the external Red Hat Quay container registry, &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt;. When the developer pushes the changes into the Git repository, all these actions trigger.&lt;/p&gt; &lt;p&gt;This is a series of complete CI/CD pipelines on OpenShift using the Jenkins and Red Hat Ansible Automation Platform. We will cover the following topics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Part 1: Continuous integration with Jenkins on OpenShift&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-employ-continuous-deployment-ansible-openshift"&gt;Continuous deployment using Ansible Automation Platform on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-manual-intervention-pipeline-restricts-deployment"&gt;How a manual intervention pipeline restricts deployment&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This article is based on the assumption that you have basic knowledge of Jenkins, OpenShift, and Ansible Automation Platform. You will also need administrator privileges for your Openshift cluster.&lt;/p&gt; &lt;h2&gt;The CI pipeline architecture&lt;/h2&gt; &lt;p&gt;The developer commits and pushes the changes after initiating the action, as shown in the architecture diagram (Figure 1). Jenkins will detect the changes with the help of polling or webhooks. We build the image in the OpenShift cluster and push it to the Quay.io container registry using buildconfig.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_0.jpg?itok=soqNMQ_1" width="600" height="275" alt="A diagram of continuous integration architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: A continuous integration architecture diagram.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Install Jenkins on OpenShift&lt;/h2&gt; &lt;p&gt;Now we need Jenkins Dashboard, which will run the CI pipeline. The easiest way is to deploy a pod of Jenkins on OpenShift from the developer's catalog, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2.jpg?itok=iU5A8rKt" width="600" height="292" alt="A screenshot of the Developer Catalog from where you can install Jenkins." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Install Jenkins from the Developer Catalog.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Follow these six steps to install Jenkins on OpenShift:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;From OpenShift Web Console, switch to the &lt;strong&gt;Developer&lt;/strong&gt; perspective and navigate to the &lt;strong&gt;Topology&lt;/strong&gt; view. Click on +Add &gt; From Developer Catalog &gt; All services&lt;/li&gt; &lt;li&gt;Search for Jenkins.&lt;/li&gt; &lt;li&gt;Select the persistence Jenkins and install it (For this article, I am keeping the settings as default, but you can modify the settings as per your requirement).&lt;/li&gt; &lt;li&gt;After installation, one Jenkins pod should appear in the project.&lt;/li&gt; &lt;li&gt;To access the dashboard of Jenkins, click on the route icon.&lt;/li&gt; &lt;li&gt;For Jenkins dashboard access, you can use the OpenShift console credential, as shown in Figure 3.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3.jpg?itok=r5VKrDaY" width="600" height="362" alt="The Jenkins login screen with OpenShift credentials." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The Jenkins login screen with OpenShift credentials.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Set up Jenkins CI pipeline&lt;/h2&gt; &lt;p&gt;The continuous integration stage consists of building and pushing the image into the container registry.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Make sure to have one git repository in place, including the Dockerfile and application dependencies like the requirements.txt file.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a Jenkinsfile with the following contents and add this jenkinsfile to the git repository.&lt;/li&gt; &lt;/ol&gt;&lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pipeline {     agent any     stages {         stage('Hello') {             steps {                 echo 'Hello World'             }         }         stage("Checkout") {             steps {                 checkout scm             }         }         stage("Docker Build") {             steps {               sh '''                   #oc start-build --from-build=&lt;build_name&gt;                   oc start-build -F red-api --from-dir=./api/               '''             }         }     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt; &lt;p class="Indent1"&gt;3. Next, create a BuildConfig file using the following content that will build the source code to executable and push.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: build.OpenShift.io/v1 kind: BuildConfig metadata:   labels:     app.kubernetes.io/name: red-api # your application name   name: red-api # your application name spec:   output:     to:       kind: DockerImage       name: ***************** # add yourimage   source:     # Expect a local directory to be streamed to OpenShift as a build source     type: Binary     binary: {}   strategy:     type: Docker     dockerStrategy:       # Find the image build instructions in./Dockerfile       dockerfilePath: Dockerfile&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;Ex: name: quay.io/&lt;username&gt;/cd:latest&lt;/p&gt; &lt;p class="Indent1"&gt;4. Next, create the secrets which will help our build config to push our recently built image in the container registry.&lt;/p&gt; &lt;p class="Indent1"&gt;5. To create a secret, type the following command into your terminal and make sure to use the username and password of your environment. For this exercise, we are using a &lt;a href="https://quay.io/"&gt;&lt;u&gt;Quay.io&lt;/u&gt;&lt;/a&gt; container registry.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret docker-registry my-secret --docker-server=quay.io --docker-username=xxxx  --docker-password=xxx &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc secrets link builder my-secret --for=mount&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;6. Next, we will create a pipeline from the Jenkins dashboard (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4.jpg?itok=h6_F-p21" width="600" height="282" alt="A screenshot of the Jenkins dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The Jenkins dashboard​​​​​ after installation.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;7. Select &lt;strong&gt;Pipeline&lt;/strong&gt; from &lt;strong&gt;New Item&lt;/strong&gt; and give a name to that pipeline.&lt;/p&gt; &lt;p class="Indent1"&gt;8. Select the &lt;strong&gt;Build Triggers&lt;/strong&gt; when the changes are pushed in the GitHub repository.&lt;/p&gt; &lt;p class="Indent1"&gt;9. In the pipeline, select &lt;strong&gt;Pipeline Script&lt;/strong&gt; from &lt;a href="https://github.com/redhat-developer-demos/ansible-automation-platform-continous-delivery-demo"&gt;&lt;u&gt;SCM&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;10. Fill in the details according to the snapshot shown in Figure 5 for the config we're using for this article.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_3.png?itok=q_BJHr_I" width="600" height="284" alt="Ansible Automation Platform integrate with jenkins pipeline​​​​​​." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Integrate Ansible Automation Platform with Jenkins pipeline​​​​​​.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;11. Once the pipeline is ready, execute it by clicking the &lt;strong&gt;Build Now&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;button. You can see a glimpse of the pipeline in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/6.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/6.jpg?itok=Jt15EUxw" width="600" height="284" alt="A screenshot showing the completed CI stage and the CI pipeline running in the Jenkins dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: The CI pipeline running in the Jenkins dashboard.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;What’s Next?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-employ-continuous-deployment-ansible-openshift"&gt;next article&lt;/a&gt; is based on the continuous deployment using the Ansible Automation platform on the OpenShift cluster. You will learn how to install the Ansible Automation platform using the operator’s hub on OpenShift and also the integration of Jenkins and Ansible Automation platform. Check out a &lt;a href="https://developers.redhat.com/devnation/devnationday-india-2022/ci-cd-ansible-automation-platform"&gt;demo&lt;/a&gt; of this project in DevNation2022.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/28/how-use-continuous-integration-jenkins-openshift" title="How to use continuous integration with Jenkins on OpenShift"&gt;How to use continuous integration with Jenkins on OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-02-28T07:00:00Z</dc:date></entry><entry><title>Edge computing: From 30 tons to 30 grams</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/27/edge-computing-101" /><author><name>Don Schenck</name></author><id>72574f1e-f86f-458b-b8df-38c160d05879</id><updated>2023-02-27T07:00:00Z</updated><published>2023-02-27T07:00:00Z</published><summary type="html">&lt;p&gt;When the ENIAC computer was introduced in 1946, it was housed in a huge room—1,800 square feet—and weighed 30 tons. It had to be assembled in place, and it wasn't going to be moved. The era of electronic computers had arrived, but only for an elite few. The idea of &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt; was science fiction—unbelievable science fiction at that. My, how things have changed.&lt;/p&gt; &lt;h2&gt;Mainframes&lt;/h2&gt; &lt;p&gt;The IBM mainframe computers, introduced in 1952, became the standard of computing for corporations and government agencies in the 1960s and 1970s. Those of us old enough can remember, for example, getting their home water bill in the form of a punched card with the words "&lt;a href="https://en.wikipedia.org/wiki/Punched_card#Do_Not_Fold,_Spindle_or_Mutilate"&gt;Do not fold, spindle or mutilate&lt;/a&gt;" on it. These mainframe computers moved processing to the corporate headquarters. Sales from cash registers, for example, would be sent to headquarters on punched paper tape where it could be read into the mainframes for reporting.&lt;/p&gt; &lt;p&gt;(Author's note: My first job in IT was processing paper tape into a mainframe.)&lt;/p&gt; &lt;h2&gt;Midrange computers&lt;/h2&gt; &lt;p&gt;In the 1970s, minicomputers (also called midrange computers) became very popular. The Digital VAX, Data General Nova, and the hugely popular IBM System/3x-400 series (System/3, System/32, System/34/, System/36, System/38, and AS/400) moved computing power even closer to the action. Midrange systems started in air-conditioned rooms with raised flooring, then moved to corners in offices, then eventually under desks as they grew in power and shrunk in physical size. Remote offices and small businesses now had a computer in their building. Larger midrange computers started to move into the spaces formerly occupied by the mainframes.&lt;/p&gt; &lt;h2&gt;The PC&lt;/h2&gt; &lt;p&gt;The 1980s saw the dawn of the desktop PC, and the trend of computing moving closer and closer to the action continued. A small desktop PC, for example, could be integrated into a production environment on a factory floor to record data and control machines. The PC would, typically, send the data to a host—often a midrange computer—and, likewise, get data from the host. This happened over a network: Twinax or ethernet cabling and the associated protocol—SNA, Novell Netware, and ethernet were the choices.&lt;/p&gt; &lt;h2&gt;Portables&lt;/h2&gt; &lt;p&gt;Adam Osborne brought the popular portable PC to the world in 1981 with the 24.5-pound Osborne 1. While more "luggable" than portable, this enabled the computer to move around more easily. Again, the processing power was moving closer to the action.&lt;/p&gt; &lt;p&gt;Laptops and notebook computers followed and continue to evolve to this day, with a WiFi connection now being a requirement.&lt;/p&gt; &lt;h2&gt;Tablets and phones&lt;/h2&gt; &lt;p&gt;In the early 1990s, the personal digital assistant, or PDA, arrived. The star was the Palm Pilot, a small device that could store and record information. Users could take notes, make voice recordings, or manage their schedules. This machine was synched to a PC via a cable. This meant, while not real-time, processing could now be carried around in a pocket.&lt;/p&gt; &lt;p&gt;The PDA was the genesis of and gave way to today's "must-have" item, the smartphone. Using WiFi and wireless 5G technology, the smartphone enables real-time data processing in a small form factor. This is one example of computing at the edge.&lt;/p&gt; &lt;p&gt;Likewise, small tablets such as the iPad allow users to carry processing power with them, along with ample screen real estate and advanced communication functionality.&lt;/p&gt; &lt;h2&gt;Embedded systems and closer to the edge&lt;/h2&gt; &lt;p&gt;But it goes further, and deeper.&lt;/p&gt; &lt;p&gt;Very small sensors and controllers can now be embedded into everyday items. These &lt;a href="https://developers.redhat.com/topics/iot"&gt;Internet of Things (IoT)&lt;/a&gt; devices range from thermostats to watches—the Apple watch weighs in at just over 30 grams, hence the title of this article—to, well, almost everything. You can even buy a ring that senses and reports data.&lt;/p&gt; &lt;p&gt;These IoT devices might (or might not) have some computing ability beyond just collecting data, but they do often communicate with an edge computer or a cloud-based system that has advanced capabilities. Processing that occurs at or near the edge is where the greatest challenges are presented. Things such as enforcing security and installing updates are paramount because this is where the action takes place.&lt;/p&gt; &lt;p&gt;All of this makes up "the edge." Sensing and processing move closer and closer to where the events occur. Communication capabilities have also improved, with WiFi, 5G, NFC, and more making it easier and more likely that edge devices will communicate with &lt;em&gt;each other&lt;/em&gt;. An in-car network, for example, can improve automotive travel; &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-and-general-motors-collaborate-trailblaze-future-software-defined-vehicles"&gt;Red Hat is working with GM on that very technology&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Edge computing example: The modern automobile&lt;/h2&gt; &lt;p&gt;Let's consider the advanced automobile as a use case for edge computing.&lt;/p&gt; &lt;p&gt;Sensors are used to report current speed, location, road conditions, outside temperature, lane edges, surrounding vehicles, and much more. These readings are reported to the driver and the drivetrain. The driver can use the information to make decisions, while an onboard computer can use the data to make adjustments—keep the car within the lanes, reduce speed based on front-facing radar, reduce torque based on road conditions, and much more.&lt;/p&gt; &lt;p&gt;They could be expanded when "smart highways" are introduced. Sensors can keep track of traffic density and speed; accidents can be used as data, reporting to cars to determine speeds and, perhaps, a new route based on congestion.&lt;/p&gt; &lt;p&gt;All this edge computing will need to be secure, and systems will need to be updated. We already have cars that can receive software updates while offline, i.e., not on the road. I can check the fuel level of my MINI from my smartphone.&lt;/p&gt; &lt;h2&gt;The future: Bright or dark?&lt;/h2&gt; &lt;p&gt;This is all just a start, only the beginning of more advanced systems running closer and closer to the action. One can easily wonder: Is Kurzweil's Singularity at hand?&lt;/p&gt; &lt;p&gt;The future, truly, is at the edge.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/27/edge-computing-101" title="Edge computing: From 30 tons to 30 grams"&gt;Edge computing: From 30 tons to 30 grams&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2023-02-27T07:00:00Z</dc:date></entry><entry><title>6 tutorials for modern app development</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/24/6-tutorials-modern-app-development" /><author><name>Jaya Christina Baskaran</name></author><id>e7e66ad7-43cf-42af-9ac0-4cd5afd54662</id><updated>2023-02-24T07:00:00Z</updated><published>2023-02-24T07:00:00Z</published><summary type="html">&lt;p&gt;Technologies like application and API connectivity accelerate the development and delivery of business solutions so you can spend more time innovating and driving competitive differentiation. As a modern application developer, you can use these features to develop skills that will set you apart from your competitors and increase the efficiency of your work. &lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/app-dev-platform"&gt;Red Hat Application Foundations&lt;/a&gt; offers a comprehensive set of components to help developers develop and modernize application software. It&lt;span&gt; is designed to help build, deploy, and operate applications with security in mind and at scale across the hybrid cloud. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;You can use the technology with applications that run on-premises or in the cloud. When combined with &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt; Red Hat OpenShift&lt;/a&gt;&lt;span&gt;, Red Hat Application Foundations creates a &lt;a href="https://developers.redhat.com/app-dev-platform"&gt;platform&lt;/a&gt; that streamlines execution across the entire application life cycle.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;This article presents quick tutorials, guides, and solution patterns that can help you gain a deeper understanding of the following technologies:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Application and API connectivity&lt;/li&gt; &lt;li aria-level="1"&gt;Data transformation&lt;/li&gt; &lt;li aria-level="1"&gt;Service composition and orchestration&lt;/li&gt; &lt;li aria-level="1"&gt;Real-time messaging and data streaming&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. An extensive Kafka learning path&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/learn/"&gt;Red Hat Developer's learning paths&lt;/a&gt; can teach you all levels of &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;, from a fundamental, basic level to developing an application that uses the Kafka Streams DSL (Domain Specific Language). The step-by-step guides span anywhere from 30 minutes to 1 hour in length.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/learn/openshift-streams-for-apache-kafka"&gt;Learn Apache Kafka by doing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;2. Enhance applications with Application Services&lt;/h2&gt; &lt;p&gt;You can use Kafka to build &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt; applications. This solution pattern introduces a streaming platform (Apache Kafka) to generate a stream of user activity events from the application web user interface (UI).&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-enhancing-applications"&gt;https://redhat-solution-patterns.github.io/solution-pattern-enhancing-applications &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Read more about &lt;a href="https://developers.redhat.com/topics/red-hat-architecture-and-design-patterns"&gt;Red Hat's architecture and design patterns&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;3. Use change data capture for stack modernization&lt;/h2&gt; &lt;p&gt;This solution pattern brings an architectural solution for scenarios where service integration must happen through data integration and cause no impact to the existing stack.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-modernization-cdc"&gt;https://redhat-solution-patterns.github.io/solution-pattern-modernization-cdc &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;4. Boost Apache Camel performance on Quarkus&lt;/h2&gt; &lt;p&gt;Check out how easy it is to create a Camel &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; project and experience the significant performance benefits for yourself.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus#"&gt;Boost Apache Camel performance on Quarkus &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;5. A step-by-step guide to Camel K basics&lt;/h2&gt; &lt;p&gt;New to &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt;? Apache Camel K is a lightweight integration framework built from Apache Camel that runs natively on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and is specifically designed for &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservice&lt;/a&gt; architectures. Learn more about how to install and run Camel K with this guide.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/courses/camel-k/basics"&gt;Learn the basics of Camel K&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;6. A 4-step learning path for API management&lt;/h2&gt; &lt;p&gt;Proper &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API management&lt;/a&gt; is extremely beneficial to an organization, but APIs can be tricky to work with. Red Hat has just the solution.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/rhoam-step-by-step"&gt;Red Hat OpenShift API Management learning &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Empower yourself! Focus on high-value initiatives by quickly adding new capabilities to your applications with these learning paths, rather than focusing on managing and maintaining complex platforms. Offload tedious responsibilities with Red Hat Application Foundations.&lt;/p&gt; &lt;p&gt;Leave a comment to let us know what tutorials you would like to see more of!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/24/6-tutorials-modern-app-development" title="6 tutorials for modern app development"&gt;6 tutorials for modern app development&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jaya Christina Baskaran</dc:creator><dc:date>2023-02-24T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 08 September 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-02-24.html" /><category term="quarkus" /><category term="kogito" /><category term="vertx" /><category term="drools" /><category term="cloudevents" /><category term="hibernate" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-02-24.html</id><updated>2023-02-24T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kogito, vertx, drools, cloudevents, hibernate"&gt; &lt;h1&gt;This Week in JBoss - 08 September 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welocme back everyone! I don’t know about where you happen to be, but where I’m living, we had the second largest snow storm in recorded weather history. I’ve been digging out my driveway and front door for a couple of days. Needless to say, it has been a bit cold here.&lt;/p&gt; &lt;p&gt;Also, my thoughts go out to those affected by the war between Russia and Ukraine, on this one year anniversary. Please stay as safe as you can.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/kogito-1-33-0-released.html"&gt;Kogito 1.33.0&lt;/a&gt; - This release includes some version bumps, further integrations, and of course, some bug fixes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-3-8/"&gt;Eclipse Vert.x 4.3.8&lt;/a&gt; - Mostly a bug fix release, but if you’re running Vert.x on Windows, it contains a fix for &lt;a href="https://github.com/vert-x3/vertx-web/security/advisories/GHSA-53jx-vvf9-4x38"&gt;CVE-2023-24815&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-16-2-final-released/"&gt;Quarkus 2.16.2.Final&lt;/a&gt; - Some bugfixes and documentation improvements mostly.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html"&gt;Cloudevents Labeling and Classification with Drools&lt;/a&gt; - Matteo demos using declarative logic, persistence, eventing, and other tech to efficiently label and store Cloudevents.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/"&gt;DataCater uses Quarkus to make Data Streaming more accessible&lt;/a&gt; - Stefan explores DataCater and their reasoning in chosing to use Quarkus to replace their usage of Play! framework in reworking their data pipeline.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/jpastreamer-extension/"&gt;Express Hibernate Queries as Type-Safe Java Streams&lt;/a&gt; - Julia discusses the library &lt;code&gt;JPAStreamer.&lt;/code&gt; It allows you to write persistence queries in a more stream-like manner and also making the queries performant.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html"&gt;Serverless Workflow Integration with Camel Routes&lt;/a&gt; - Ricardo demonstrates the integration of Camel Routes with Kogito Serverless Workflow.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/newsletter/29/"&gt;Quarkus Newsletter #29&lt;/a&gt; - Monthly Quarkus Newsletter for more Quarkus related information&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/jboss-maven/apache-maven-faqs/"&gt;Apache Maven Faqs for Java Developers&lt;/a&gt; - If you’re new to Apache Maven, or simply have some questions, this quick FAQ may help you out.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=H9yK0xnExeA"&gt;Quarkus Insights 118&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_until_next_time"&gt;Until next time!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Again, everyone stay safe, and I hope that your tests stay green!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry><entry><title>Kubernetes-Native Development with Quarkus and Eclipse JKube</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/kubernetes-native-development-with-quarkus-and-eclipse-jkube/&#xA;            " /><author><name>Eric Deandrea (https://twitter.com/edeandrea)</name></author><id>https://quarkus.io/blog/kubernetes-native-development-with-quarkus-and-eclipse-jkube/</id><updated>2023-02-23T00:00:00Z</updated><published>2023-02-23T00:00:00Z</published><summary type="html">Table of Contents Introduction What is Eclipse JKube Remote Development? Eclipse JKube Remote Development and Quarkus Wrap-Up This article explains what Eclipse JKube Remote Development is and how can it help developers build Kubernetes-native applications with Quarkus. Introduction As mentioned in my previous article, Kubernetes-native inner loop development with Quarkus,...</summary><dc:creator>Eric Deandrea (https://twitter.com/edeandrea)</dc:creator><dc:date>2023-02-23T00:00:00Z</dc:date></entry><entry><title>Our advice for installing Node.js modules using npm registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/22/installing-nodejs-modules-using-npm-registry" /><author><name>Lucas Holmquist</name></author><id>5b80b94f-8203-4be8-9808-b6c602819ac1</id><updated>2023-02-22T07:00:00Z</updated><published>2023-02-22T07:00:00Z</published><summary type="html">&lt;p&gt;The Node.js ecosystem is a vast landscape, and one of the most important parts of that ecosystem is its various modules.  Installing these modules is an easy process, thanks to the npm registry. It is easy to download packages, create your own module, and publish it to the npm registry. The Node.js Reference Architecture team has recommendations for creating and publishing your modules.&lt;/p&gt; &lt;p&gt;Follow the series:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/05/10/introduction-nodejs-reference-architecture-part-2-logging-nodejs"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Typescript&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 9: &lt;a href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications"&gt;Securing Node.js applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 10: &lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Accessibility&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 11: &lt;a href="https://developers.redhat.com/articles/2022/12/21/typical-development-workflows"&gt;Typical development workflows&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 12: Npm development&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Package development&lt;/h2&gt; &lt;p&gt;When starting the creation of a new package, it is recommended to use the &lt;strong&gt;npm init&lt;/strong&gt; command instead of hand coding to quickly and accurately create a new package.json. You can even accept all the defaults by providing the &lt;strong&gt;-y &lt;/strong&gt;flag when running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm init -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command can be modified to tailor the results to your specific organization's needs. To read more about this feature, check out the &lt;a href="https://docs.npmjs.com/cli/v9/commands/npm-init"&gt;official npm documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;While &lt;strong&gt;name&lt;/strong&gt; and &lt;strong&gt;version&lt;/strong&gt; are the only fields that are required to publish a package, the team recommends completing a few other fields to provide more information to the user. The following sections provide details about those fields as well as other recommendations. To see the full list, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/npm-package-development.md#packagejson"&gt;npm package development section&lt;/a&gt; of the Node.js Reference Architecture.&lt;/p&gt; &lt;h3&gt;The files field&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;files&lt;/strong&gt; field is a list of files that you want to be published with your package. This field is handy when using a bundler to transpile code, and you only want to include that transpiled code. The team recommends not to include tests, which should reduce the package size. But you can include docs. For a list of the files that are automatically included, check out the &lt;a href="https://docs.npmjs.com/cli/v8/configuring-npm/package-json#files"&gt;npm docs&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;The support field&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;support&lt;/strong&gt; field will help package maintainers communicate with and set expectations for their users about the level of support they are willing to provide on a package. The team has worked closely with the &lt;a href="https://github.com/nodejs/package-maintenance"&gt;Node.js Package Maintenance Team&lt;/a&gt; on their recommendations for what this support field should look like. For our &lt;a href="https://github.com/nodeshift/opossum"&gt;opossum&lt;/a&gt; module, we &lt;a href="https://github.com/nodeshift/opossum/blob/main/package.json#L75"&gt;set this field&lt;/a&gt; to true and supplied our support information in a separate &lt;a href="https://github.com/nodeshift/opossum/blob/main/package-support.json"&gt;package-support.json file&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information on the support field, read the &lt;a href="https://github.com/nodejs/package-maintenance/blob/main/docs/PACKAGE-SUPPORT.md"&gt;Package Maintenance Team statements&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;The type field&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;type&lt;/strong&gt; field defines the module format that Node.js will use. For ES Modules (ESM), use &lt;strong&gt;module.&lt;/strong&gt; For Common JS (CJS) modules, use &lt;strong&gt;commonjs&lt;/strong&gt;.  Adding this  For more information on how Node.js determines the difference between ESM and CJS modules take a &lt;a href="https://nodejs.org/dist/latest-v18.x/docs/api/packages.html#determining-module-system"&gt;look at the Node.js official docs here&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Specify a license&lt;/h3&gt; &lt;p&gt;You should specify a license for your package so that people know how they are permitted to use it and any restrictions you place on it. The team has the most experience with using both the MIT and Apache-2 licenses. But if you are publishing a package on behalf of a company, it is recommended to consult your organization about their preferred licenses.&lt;/p&gt; &lt;p&gt;If a package is going to be unlicensed, it is recommended to set the private field to &lt;strong&gt;true. &lt;/strong&gt;You can &lt;a href="https://docs.npmjs.com/cli/v8/configuring-npm/package-json#license"&gt;find more information&lt;/a&gt; on licenses and how they relate to the package.json. &lt;/p&gt; &lt;h3&gt;The main field&lt;/h3&gt; &lt;p&gt;This is the primary entry point to your package that should be a module relative to the root of your package folder. Most of the time, the default will be &lt;strong&gt;index.js&lt;/strong&gt; if the &lt;strong&gt;main&lt;/strong&gt; field is not set.&lt;/p&gt; &lt;h3&gt;The scripts property&lt;/h3&gt; &lt;p&gt;This property is an array containing script commands run at various times in the lifecycle of your package. The key is the lifecycle event, and the value is the command to run at that point. The team recommends creating scripts that handle calling the tests, linters, and any build steps that might need to occur. The team recommends not using a postInstall script if possible, as this could be a security risk.&lt;/p&gt; &lt;h3&gt;Advice for choosing dependencies&lt;/h3&gt; &lt;p&gt;Another important part of package development is the dependencies. The team has created a resource for &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/dependencies"&gt;choosing and vetting dependencies&lt;/a&gt;. This is also discussed in the &lt;a href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications#1__choosing_third_party_dependencies"&gt;8 elements of securing Node.js applications&lt;/a&gt; article.&lt;/p&gt; &lt;p&gt;During package development, it is helpful to add &lt;strong&gt;package-lock.json&lt;/strong&gt; to source control, so all developers working on the package can install the same versions of dependencies. It is also recommended to know what semver range you are specifying for any dependency you wish to install.&lt;/p&gt; &lt;p&gt;Here are a few examples:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;strong&gt;^&lt;/strong&gt; (carrot) will give you all the minor and patch releases when they become available.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;~ &lt;/strong&gt;(tilde) will include everything greater than a particular version in the same minor range.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://semver.npmjs.com/"&gt;semver calculator&lt;/a&gt; is a great resource to assist in making this choice.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Recommendations for publishing modules to the npm registry&lt;/h2&gt; &lt;p&gt;To publish modules to npm, the team recommends that you turned on 2-factor authentication and use automation tokens for those modules published with some type of Continuous Integration/Continuous Delivery workflow. The following sections provide recommendations for preparing your code, transpiling sources, and module versioning.&lt;/p&gt; &lt;h3&gt;Preparing code&lt;/h3&gt; &lt;p&gt;The team believes that it is important to keep your published packages tidy. Here are a couple ways to do this:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Specify ignore files in a &lt;strong&gt;.npmignore.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Specify the files you only want to publish with the &lt;strong&gt;files &lt;/strong&gt;property in your package.json.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;As for tests and documentation, the team has no clear recommendations. For modules our team publishes, we exclude the tests, and we publish any API docs that go beyond README to GitHub pages.&lt;/p&gt; &lt;h3&gt;Transpiling sources&lt;/h3&gt; &lt;p&gt;A package written in another language like TypeScript should be published with transpiled JavaScript as the primary executable.&lt;/p&gt; &lt;p&gt;Generally, the original sources should not be required to use your published package, but it's your preference whether to publish them or not. Similar to the guidance on tests, the modules that the team publishes that are transpiled only contain the generated code.&lt;/p&gt; &lt;p&gt;If your module requires build steps, it is recommended to run those before you publish and not when the user installs the package. This is where you would use the &lt;strong&gt;prePublish&lt;/strong&gt; script as mentioned above.&lt;/p&gt; &lt;p&gt;It is important, if you are using Typescript or generating type definitions for your package, that you add a reference to the generated type file in your package.json and ensure that the file is included when publishing.&lt;/p&gt; &lt;h3&gt;Module versions&lt;/h3&gt; &lt;p&gt;We recommend using &lt;a href="https://semver.org/"&gt;Semantic Versioning&lt;/a&gt; when possible. This will make it easier for users to determine if a new module version will potentially break their code.&lt;/p&gt; &lt;p&gt;There are two automation tools that can help with bumping your module to the correct version. These tools will base the version bump on commit messages that follow the &lt;a href="https://www.conventionalcommits.org/en/v1.0.0/"&gt;conventional commit standard&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/googleapis/release-please"&gt;release-please&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/conventional-changelog/standard-version"&gt;standard-version&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The npm CLI also provides a &lt;a href="https://docs.npmjs.com/cli/v7/commands/npm-version"&gt;version command&lt;/a&gt; that will increase the package version. You can find all the recommendations for publishing on the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/npm-publishing.md"&gt;Npm Publishing Guidelines GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The npm proxy/mirror technique&lt;/h2&gt; &lt;p&gt;The npm registry is great because it allows users to easily install third-party modules with just a few commands. But there could be some additional concerns if you are part of an organization that limits internet access or you are worried that a module you depend on could potentially disappear from the public registry.&lt;/p&gt; &lt;p&gt;This is where having a layer between your organization and the public npm registry can help. This is commonly referred to as an npm proxy/npm mirror.&lt;/p&gt; &lt;p&gt;In these situations, the team recommends using a proxy/mirror when possible. The following is a list of various scenarios where you might consider using this technique:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You need to limit the installation of modules to only a specific set.&lt;/li&gt; &lt;li&gt;You have limited network access.&lt;/li&gt; &lt;li&gt;Using a proxy/mirror can provide a centralized point for scanning security vulnerabilities.&lt;/li&gt; &lt;li&gt;A mirror can reduce the dependency on the public registry.&lt;/li&gt; &lt;li&gt;You need to maintain a copy of a module in case it is removed from the public registry.&lt;/li&gt; &lt;li&gt;You are a good npm citizen. The public registry is a free service, and npm allows &lt;a href="https://blog.npmjs.org/post/187698412060/acceptible-use"&gt;updates to 5 million requests per month&lt;/a&gt;, which can be used up quickly with CI builds.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The proxy/mirror techniques can be used in two ways:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;For a more global solution, set the registry using the npm CLI:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;npm set registry URL&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Use a &lt;strong&gt;.npmrc&lt;/strong&gt; file per project if you need more fine-grained control.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can find more information about npm proxies and mirrors &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/npm-proxy.md"&gt;in this section of Node.js Reference Architecture&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;h2&gt;The npm registry makes installing modules easy&lt;/h2&gt; &lt;p&gt;As previously mentioned, this is just a subset of the full list of our team's advice and recommendations for installing and publishing modules to the npm registry. For the full list of recommendations, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/npm-package-development.md#packagejson"&gt;npm package development section of the Node.js Reference Architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you can view our work. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/22/installing-nodejs-modules-using-npm-registry" title="Our advice for installing Node.js modules using npm registry"&gt;Our advice for installing Node.js modules using npm registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2023-02-22T07:00:00Z</dc:date></entry><entry><title>How to install Microsoft SQL on RHEL using ansible-navigator</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/21/5-steps-install-microsoft-sql-rhel8-automation" /><author><name>Nagesh Rathod</name></author><id>08fd5305-d0a4-469d-b86b-b07ee8f796a3</id><updated>2023-02-21T07:00:00Z</updated><published>2023-02-21T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I will show you how to install Microsoft SQL on a dedicated &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 8 instance and pass the execution environment image while the Ansible playbook runs. We will also explore how using the &lt;a href="https://www.ansible.com/blog/released-automation-content-navigator-2.0"&gt;automation content navigator&lt;/a&gt; is an efficient method for this installation.&lt;/p&gt; &lt;h2&gt;5 steps to install Microsoft SQL on RHEL using automation content navigator&lt;/h2&gt; &lt;p&gt;Before getting started, please make sure that you have installed the &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; on your machine. Otherwise, please refer to the previous &lt;a href="https://developers.redhat.com/articles/2023/01/01/how-install-red-hat-ansible-automation-platform-rhel-9#"&gt;article&lt;/a&gt; about the Ansible Automation Platform installation.&lt;/p&gt; &lt;h3&gt;1. Setting up automation content navigator&lt;/h3&gt; &lt;p&gt;The automation content navigator is a text-based user interface (TUI) tool for creating, reviewing, and troubleshooting Ansible content, including inventories, playbooks, and collections.&lt;/p&gt; &lt;p&gt;To run the automation content navigator on RHEL8, you must have superuser privileges, ansible-core, &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; 3, and &lt;a href="https://developers.redhat.com/videos/youtube/bJDI_QuXeCE"&gt;Podman&lt;/a&gt; installed.&lt;/p&gt; &lt;p&gt;Verify that Ansible Automation Platform is installed on your environment by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​[redhat@redhat]$ ansible --version&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will see the following output verifying the installation and version:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible [core 2.13.3]   config file = /etc/ansible/ansible.cfg   configured module search path = ['/home/nagesh/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']   ansible python module location = /usr/lib/python3.9/site-packages/ansible   ansible collection location = /home/nagesh/.ansible/collections:/usr/share/ansible/collections   executable location = /usr/bin/ansible   python version = 3.9.14 [GCC 11.3.1 20220421 (Red Hat 11.3.1-2)]   jinja version = 3.1.2   libyaml = True&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, install a Python package that will be used to install the automation content navigator.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf install python3-pip&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2. Install the automation content navigator&lt;/h3&gt; &lt;p&gt;Install automation content navigator with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;python3 -m pip install ansible-navigator --user&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the installation path to the user’s shell initialization file and source it by entering the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;echo 'export PATH=$HOME/.local/bin:$PATH' &gt;&gt; ~/.profile&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;source ~/.profile&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3. Launch the automation content navigator&lt;/h3&gt; &lt;p&gt;Launch the automation content navigator TUI by entering the following(Figure 1):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-navigator&lt;/code&gt;&lt;/pre&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/navigator-min.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/navigator-min.png?itok=ugj4BNUI" width="600" height="307" alt="The automation content navigator TUI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The automation content navigator TUI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;4. Setting up the Ansible execution environment&lt;/h3&gt; &lt;p&gt;Execution environments are &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images that help execute Ansible playbooks. Automation can now be built and deployed using Ansible execution environments instead of Python virtual environments. Unlike legacy virtual environments, execution environments are container images that make it possible to incorporate system-level dependencies and collection-based content. Each execution environment allows you to have a customized image to run jobs, and each of them contains only what you need when running the job, nothing more.&lt;/p&gt; &lt;p&gt;A registry &lt;a href="https://access.redhat.com/terms-based-registry/"&gt;service account&lt;/a&gt; must be created prior to completing any of the subsequent tasks. Create a service account by entering the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login registry.redhat.io Username: {REGISTRY-SERVICE-ACCOUNT-USERNAME} Password: {REGISTRY-SERVICE-ACCOUNT-PASSWORD} Login Succeeded!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Container file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​FROM registry.redhat.io/ansible-automation-platform-22/ee-29-rhel8:latest RUN ansible-galaxy collection install microsoft.sql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Build an image by entering the following &lt;a href="https://developers.redhat.com/blog/2020/11/19/transitioning-from-docker-to-podman#"&gt;Podman&lt;/a&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​podman build -t &lt;image-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before pushing, make sure you are logged in to your private container image registry using the podman login command. &lt;/p&gt; &lt;p&gt;Push the image into the container image registry as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​podman push &lt;image-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;5. Install Microsoft SQL Server&lt;/h3&gt; &lt;p&gt;Using the &lt;code&gt;ansible-navigator&lt;/code&gt; command, run the following playbook, and define the executive environment image by using the &lt;code&gt;--eei&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;Playbook: microsoft_sql_playbook.yaml.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - hosts: dev   become: yes   vars:     mssql_accept_microsoft_odbc_driver_17_for_sql_server_eula: true     mssql_accept_microsoft_cli_utilities_for_sql_server_eula: true     mssql_accept_microsoft_sql_server_standard_eula: true     mssql_password: "123@Redhat"     mssql_edition: Evaluation     mssql_enable_sql_agent: true     mssql_install_fts: true     mssql_install_powershell: true     mssql_tune_for_fua_storage: true   roles:     - microsoft.sql.server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inventory file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[dev] &lt;target host IP&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the &lt;code&gt;ansible-navigator&lt;/code&gt; command to execute with dependencies as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-navigator run -m stdout mssql-install.yaml -i inventory  --user ec2-user --key-file redhat  --eei quay.io/narathod/mssql-aap:0.2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;TASK [microsoft.sql.server : Configure a listener for the  availability group] *** skipping: [3.236.15.223] TASK [microsoft.sql.server : Ensure the ansible_managed header in /var/opt/mssql/mssql.conf] *** changed: [3.236.15.223] RUNNING HANDLER [microsoft.sql.server : Restart the mssql-server service] ****** changed: [3.236.15.223] TASK [microsoft.sql.server : Post-input SQL scripts to SQL Server] ************* PLAY RECAP ********************************************************************* 3.236.15.223               : ok=39   changed=16   unreachable=0    failed=0    skipped=72   rescued=0    ignored=0   &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Use automation to install databases on RHEL&lt;/h2&gt; &lt;p&gt;This article demonstrated how you can use &lt;a href="developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; to install Microsoft SQL Server on RHEL 8 machines via the automation content navigator. You can use this method to install other databases such as MongoDB, PostgreSQL, and OracleDB by picking the Ansible role and playbooks for that database. The rest stays the same.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with the Ansible Automation Platform by exploring interactive labs. Ansible Automation Platform is also available as a managed offering on&lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt; Microsoft Azure&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;AWS&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/21/5-steps-install-microsoft-sql-rhel8-automation" title="How to install Microsoft SQL on RHEL using ansible-navigator"&gt;How to install Microsoft SQL on RHEL using ansible-navigator&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-02-21T07:00:00Z</dc:date></entry><entry><title>Automate your SSO with Ansible and Keycloak</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/20/automate-your-sso-ansible-and-keycloak" /><author><name>Romain Pelisse</name></author><id>64dc22ed-4b7a-4843-8cf4-20eff876e7ba</id><updated>2023-02-20T07:00:00Z</updated><published>2023-02-20T07:00:00Z</published><summary type="html">&lt;p&gt;The article &lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;Deploy Keycloak single sign-on with Ansible&lt;/a&gt; discussed how to automate the deployment of Keycloak. In this follow-up article, we’ll use that as a baseline and explore how to automate the configuration of the Keycloak single sign-on (SSO) server, including setting up users, specifying LDAP connection details, and so on.&lt;/p&gt; &lt;p&gt;Here again, to facilitate our &lt;a href="http://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;, we will leverage an &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; collection named &lt;a href="https://galaxy.ansible.com/middleware_automation/keycloak"&gt;middleware_automation.keycloak&lt;/a&gt;, specifically designed for this endeavor. &lt;/p&gt; &lt;h2&gt;Install Keycloak with Ansible&lt;/h2&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;the previous article&lt;/a&gt;, we saw in detail how to automate the installation of Keycloak. For this new installment, we’ll start from there using the following playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Playbook for Keycloak Hosts hosts: keycloak vars: keycloak_admin_password: "remembertochangeme" collections: - middleware_automation.keycloak roles: - keycloak &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This short playbook will take care of the installation of the single sign-on server itself, which already includes quite a few tasks to perform on the target system, including:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creating appropriate operating system user and group accounts (the name is keycloak for both)&lt;/li&gt; &lt;li aria-level="1"&gt;Downloading the installation archive from the Keycloak website&lt;/li&gt; &lt;li aria-level="1"&gt;Unarchiving the content while ensuring that all the files are associated with the appropriate user and groups along with the correct privileges&lt;/li&gt; &lt;li aria-level="1"&gt;Ensuring that the required version of the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; Virtual Machine (JVM) is installed&lt;/li&gt; &lt;li aria-level="1"&gt;Integrating the software into the host service management system (in our case, the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;&lt;a href="https://www.linux.com/training-tutorials/understanding-and-using-systemd/"&gt; systemd&lt;/a&gt; daemon).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;However, prior to running the playbook, we are going to enhance it even further to perform day two configurations of the Keycloak server, including the configuration of the SSO realm, clients, and users.&lt;/p&gt; &lt;h2&gt;Configure single sign-on&lt;/h2&gt; &lt;p&gt;The Ansible collection for Keycloak allows defining the realm, client, and users without adding a single, extra task. All that is needed is to define a few extra variables. Of course, those variables are quite structured and need to be formatted correctly for Ansible to be able to configure Keycloak appropriately. The following is a complete, working example of such a configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Playbook for Keycloak Hosts hosts: all vars: keycloak_admin_password: "remembertochangeme" keycloak_realm: TestRealm collections: - middleware_automation.keycloak roles: - keycloak tasks: - name: Keycloak Realm Role ansible.builtin.include_role: name: keycloak_realm vars: keycloak_client_default_roles: - TestRoleAdmin - TestRoleUser keycloak_client_users: - username: TestUser password: password client_roles: - client: TestClient role: TestRoleUser realm: "{{ keycloak_realm }}" - username: TestAdmin password: password client_roles: - client: TestClient role: TestRoleUser realm: "{{ keycloak_realm }}" - client: TestClient role: TestRoleAdmin realm: "{{ keycloak_realm }}" keycloak_realm: TestRealm keycloak_clients: - name: TestClient roles: "{{ keycloak_client_default_roles }}" realm: "{{ keycloak_realm }}" public_client: "{{ keycloak_client_public }}" web_origins: "{{ keycloak_client_web_origins }}" users: "{{ keycloak_client_users }}" client_id: TestClient&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that this example, purposely, does not rely on any external sources (such as an LDAP server) so that it can be used easily, to test the collection without requiring the setup of any extra resources.&lt;/p&gt; &lt;p&gt;Because the SSO configuration is quite dense, we are going to break down each portion to not only provide additional insight, but to illustrate its significance in the SSO configuration.&lt;/p&gt; &lt;h3&gt;Define the realm&lt;/h3&gt; &lt;p&gt;The very first step is to define a realm, which, for the purpose of this article, contains the desired user and role details, but other capabilities provided by Keycloak that will be explored throughout the article. To create the realm, we just need to add one variable to our playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;… - client: TestClient role: TestRoleAdmin realm: "{{ keycloak_realm }}" keycloak_realm: TestRealm keycloak_clients: … &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure roles and users&lt;/h3&gt; &lt;p&gt;The next portion of the variables provided populates the realm with the appropriate details related to users and roles. For the demonstration of this article, we added two users (and two roles) to the realm we are defining:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;TestAdmin&lt;/code&gt;: An admin user who can connect to the SSO server and configure the realm. This user belongs to both roles we defined above.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;TestClient&lt;/code&gt;: A user belonging to the realm and thus belongs only in the &lt;code&gt;TestRoleUser&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt;… keycloak_client_default_roles: - TestRoleAdmin - TestRoleUser keycloak_client_users: - username: TestUser password: password client_roles: - client: TestClient role: TestRoleUser realm: "{{ keycloak_realm }}" - username: TestAdmin password: password client_roles: - client: TestClient role: TestRoleUser realm: "{{ keycloak_realm }}" - client: TestClient role: TestRoleAdmin realm: "{{ keycloak_realm }}" … &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Define Keycloak clients&lt;/h3&gt; &lt;p&gt;The last portion of the variables defines the client associated with the roles so that their users can use the SSO service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;… keycloak_clients: - name: TestClient roles: "{{ keycloak_client_default_roles }}" realm: "{{ keycloak_realm }}" public_client: "{{ keycloak_client_public }}" web_origins: "{{ keycloak_client_web_origins }}" users: "{{ keycloak_client_users }}" … &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Run the playbook&lt;/h2&gt; &lt;p&gt;That’s it! With these details provided, we can now run the playbook to deploy Keycloak and fully configure our SSO instance (based on the user's information inside the &lt;code&gt;TestRealm&lt;/code&gt;). Execute the following command to execute the automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ansible-playbook -i inventory keycloak.yml PLAY [Playbook for Keycloak Hosts] ********************************************* TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [keycloak : Validating arguments against arg spec 'main'] ***************** ok: [localhost] TASK [keycloak : Check prerequisites] ****************************************** included: /work/roles/keycloak/tasks/prereqs.yml for localhost TASK [keycloak : Validate admin console password] ****************************** ok: [localhost] TASK [keycloak : Validate configuration] *************************************** ok: [localhost] TASK [keycloak : Validate credentials] ***************************************** ok: [localhost] TASK [keycloak : Validate persistence configuration] *************************** skipping: [localhost] TASK [keycloak : Ensure required packages are installed] *********************** included: /work/roles/keycloak/tasks/fastpackages.yml for localhost TASK [keycloak : Check if packages are already installed] ********************** fatal: [localhost]: FAILED! =&gt; {"changed": true, "cmd": ["rpm", "-q", "java-11-openjdk-headless", "unzip", "procps-ng", "initscripts"], "delta": "0:00:00.006828", "end": "2022-12-28 14:22:44.352750", "msg": "non-zero return code", "rc": 3, "start": "2022-12-28 14:22:44.345922", "stderr": "", "stderr_lines": [], "stdout": "package java-11-openjdk-headless is not installed\npackage unzip is not installed\nprocps-ng-3.3.15-6.el8.x86_64\npackage initscripts is not installed", "stdout_lines": ["package java-11-openjdk-headless is not installed", "package unzip is not installed", "procps-ng-3.3.15-6.el8.x86_64", "package initscripts is not installed"]} TASK [keycloak : Add missing packages to the yum install list] ***************** ok: [localhost] TASK [keycloak : Install packages: ['java-11-openjdk-headless', 'unzip', 'initscripts']] *** changed: [localhost] TASK [keycloak : Include firewall config tasks] ******************************** skipping: [localhost] TASK [keycloak : Include install tasks] **************************************** included: /work/roles/keycloak/tasks/install.yml for localhost TASK [keycloak : Validate parameters] ****************************************** ok: [localhost] TASK [keycloak : Check for an existing deployment] ***************************** ok: [localhost] TASK [keycloak : Stop the old keycloak service] ******************************** skipping: [localhost] TASK [keycloak : Remove the old keycloak deployment] *************************** skipping: [localhost] TASK [keycloak : Check for an existing deployment after possible forced removal] *** ok: [localhost] TASK [keycloak : Create keycloak service user/group] *************************** changed: [localhost] TASK [keycloak : Create keycloak install location] ***************************** changed: [localhost] TASK [keycloak : Set download archive path] ************************************ ok: [localhost] TASK [keycloak : Check download archive path] ********************************** ok: [localhost] TASK [keycloak : Check local download archive path] **************************** ok: [localhost] TASK [keycloak : Download keycloak archive] ************************************ ok: [localhost] TASK [keycloak : Perform download from RHN] ************************************ skipping: [localhost] TASK [keycloak : Download rhsso archive from alternate location] *************** skipping: [localhost] TASK [keycloak : Check downloaded archive] ************************************* ok: [localhost] TASK [keycloak : Copy archive to target nodes] ********************************* changed: [localhost] TASK [keycloak : Check target directory: /opt/keycloak/keycloak-18.0.2] ******** ok: [localhost] TASK [keycloak : Extract Keycloak archive on target] *************************** changed: [localhost] TASK [keycloak : Inform decompression was not executed] ************************ skipping: [localhost] TASK [keycloak : Reown installation directory to keycloak] ********************* ok: [localhost] TASK [keycloak : Install postgres driver] ************************************** skipping: [localhost] TASK [keycloak : Deploy keycloak config to /opt/keycloak/keycloak-18.0.2/standalone/configuration/keycloak.xml from standalone.xml.j2] *** changed: [localhost] TASK [keycloak : Deploy keycloak config with remote cache store to /opt/keycloak/keycloak-18.0.2/standalone/configuration/keycloak.xml] *** skipping: [localhost] TASK [keycloak : Include systemd tasks] **************************************** included: /work/roles/keycloak/tasks/systemd.yml for localhost TASK [keycloak : Configure keycloak service script wrapper] ******************** changed: [localhost] TASK [keycloak : Determine JAVA_HOME for selected JVM RPM] ********************* ok: [localhost] TASK [keycloak : Configure sysconfig file for keycloak service] **************** changed: [localhost] TASK [keycloak : Configure systemd unit file for keycloak service] ************* changed: [localhost] TASK [keycloak : Reload systemd] *********************************************** ok: [localhost] TASK [keycloak : Start and wait for keycloak service (first node db)] ********** skipping: [localhost] TASK [keycloak : Start and wait for keycloak service (remaining nodes)] ******** included: /work/roles/keycloak/tasks/start_keycloak.yml for localhost TASK [keycloak : Start keycloak service] *************************************** changed: [localhost] TASK [keycloak : Wait until keycloak becomes active http://localhost:9990/health] *** FAILED - RETRYING: [localhost]: Wait until keycloak becomes active http://localhost:9990/health (25 retries left). ok: [localhost] TASK [keycloak : Check service status] ***************************************** ok: [localhost] TASK [keycloak : Verify service status] **************************************** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [keycloak : Flush handlers] *********************************************** RUNNING HANDLER [keycloak : Restart handler] *********************************** included: /work/roles/keycloak/tasks/restart_keycloak.yml for localhost RUNNING HANDLER [keycloak : Restart and enable {{ keycloak.service_name }} service] *** changed: [localhost] RUNNING HANDLER [keycloak : Wait until {{ keycloak.service_name }} becomes active {{ keycloak.health_url }}] *** FAILED - RETRYING: [localhost]: Wait until keycloak becomes active http://localhost:9990/health (25 retries left). ok: [localhost] RUNNING HANDLER [keycloak : Restart and enable {{ keycloak.service_name }} service] *** skipping: [localhost] TASK [keycloak : Include patch install tasks] ********************************** skipping: [localhost] TASK [keycloak : Link default logs directory] ********************************** changed: [localhost] TASK [keycloak : Check admin credentials by generating a token (supposed to fail on first installation)] *** FAILED - RETRYING: [localhost]: Check admin credentials by generating a token (supposed to fail on first installation) (2 retries left). FAILED - RETRYING: [localhost]: Check admin credentials by generating a token (supposed to fail on first installation) (1 retries left). fatal: [localhost]: FAILED! =&gt; {"attempts": 2, "cache_control": "no-store", "changed": false, "connection": "close", "content_length": "72", "content_type": "application/json", "date": "Wed, 28 Dec 2022 14:24:25 GMT", "elapsed": 0, "json": {"error": "invalid_grant", "error_description": "Invalid user credentials"}, "msg": "Status code was 401 and not [200]: HTTP Error 401: Unauthorized", "pragma": "no-cache", "redirected": false, "referrer_policy": "no-referrer", "status": 401, "strict_transport_security": "max-age=31536000; includeSubDomains", "url": "http://localhost:8080/auth/realms/master/protocol/openid-connect/token", "x_content_type_options": "nosniff", "x_frame_options": "SAMEORIGIN", "x_xss_protection": "1; mode=block"} TASK [keycloak : Create keycloak admin user] *********************************** changed: [localhost] TASK [keycloak : Restart keycloak] ********************************************* included: /work/roles/keycloak/tasks/restart_keycloak.yml for localhost TASK [keycloak : Restart and enable keycloak service] ************************** changed: [localhost] TASK [keycloak : Wait until keycloak becomes active http://localhost:9990/health] *** FAILED - RETRYING: [localhost]: Wait until keycloak becomes active http://localhost:9990/health (25 retries left). ok: [localhost] TASK [keycloak : Restart and enable keycloak service] ************************** skipping: [localhost] TASK [keycloak : Wait until keycloak becomes active http://localhost:9990/health] *** ok: [localhost] TASK [Keycloak Realm Role] ***************************************************** TASK [keycloak_realm : Validating arguments against arg spec 'main'] *********** ok: [localhost] TASK [keycloak_realm : Generate keycloak auth token] *************************** ok: [localhost] TASK [keycloak_realm : Determine if realm exists] ****************************** ok: [localhost] TASK [keycloak_realm : Create Realm] ******************************************* ok: [localhost] TASK [keycloak_realm : Create user federation] ********************************* TASK [keycloak_realm : Validate Keycloak clients] ****************************** ok: [localhost] =&gt; (item=TestClient) TASK [keycloak_realm : Create or update a Keycloak client] ********************* changed: [localhost] =&gt; (item=None) changed: [localhost] TASK [keycloak_realm : Create client roles] ************************************ included: /work/roles/keycloak_realm/tasks/manage_client_roles.yml for localhost =&gt; (item={'name': 'TestClient', 'roles': ['TestRoleAdmin', 'TestRoleUser'], 'realm': 'TestRealm', 'public_client': True, 'web_origins': '+', 'users': [{'username': 'TestUser', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}]}, {'username': 'TestAdmin', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}, {'client': 'TestClient', 'role': 'TestRoleAdmin', 'realm': 'TestRealm'}]}], 'client_id': 'TestClient'}) TASK [keycloak_realm : Create client roles] ************************************ changed: [localhost] =&gt; (item=None) changed: [localhost] =&gt; (item=None) changed: [localhost] TASK [keycloak_realm : Create client users] ************************************ included: /work/roles/keycloak_realm/tasks/manage_client_users.yml for localhost =&gt; (item={'name': 'TestClient', 'roles': ['TestRoleAdmin', 'TestRoleUser'], 'realm': 'TestRealm', 'public_client': True, 'web_origins': '+', 'users': [{'username': 'TestUser', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}]}, {'username': 'TestAdmin', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}, {'client': 'TestClient', 'role': 'TestRoleAdmin', 'realm': 'TestRealm'}]}], 'client_id': 'TestClient'}) TASK [keycloak_realm : Manage Users] ******************************************* included: /work/roles/keycloak_realm/tasks/manage_user.yml for localhost =&gt; (item={'username': 'TestUser', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}]}) included: /work/roles/keycloak_realm/tasks/manage_user.yml for localhost =&gt; (item={'username': 'TestAdmin', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}, {'client': 'TestClient', 'role': 'TestRoleAdmin', 'realm': 'TestRealm'}]}) TASK [keycloak_realm : Check if User Already Exists] *************************** ok: [localhost] TASK [keycloak_realm : Create User] ******************************************** ok: [localhost] TASK [keycloak_realm : Get User] *********************************************** ok: [localhost] TASK [keycloak_realm : Update User Password] *********************************** ok: [localhost] TASK [keycloak_realm : Check if User Already Exists] *************************** ok: [localhost] TASK [keycloak_realm : Create User] ******************************************** ok: [localhost] TASK [keycloak_realm : Get User] *********************************************** ok: [localhost] TASK [keycloak_realm : Update User Password] *********************************** ok: [localhost] TASK [keycloak_realm : Manage User Roles] ************************************** included: /work/roles/keycloak_realm/tasks/manage_user_roles.yml for localhost =&gt; (item={'username': 'TestUser', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}]}) included: /work/roles/keycloak_realm/tasks/manage_user_roles.yml for localhost =&gt; (item={'username': 'TestAdmin', 'password': 'password', 'client_roles': [{'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}, {'client': 'TestClient', 'role': 'TestRoleAdmin', 'realm': 'TestRealm'}]}) TASK [keycloak_realm : Get User TestUser] ************************************** ok: [localhost] TASK [keycloak_realm : Refresh keycloak auth token] **************************** ok: [localhost] TASK [keycloak_realm : Manage Client Role Mapping for TestUser] **************** included: /work/roles/keycloak_realm/tasks/manage_user_client_roles.yml for localhost =&gt; (item={'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}) TASK [keycloak_realm : Get Realm for role] ************************************* ok: [localhost] TASK [keycloak_realm : Check if Mapping is available] ************************** ok: [localhost] TASK [keycloak_realm : Create Role Mapping] ************************************ ok: [localhost] =&gt; (item={'id': '5cdd02f6-8341-4cd0-ba31-46631a847cdf', 'name': 'TestRoleUser', 'composite': False, 'clientRole': True, 'containerId': 'f084b840-c30d-4c93-933e-18f8be1ed19a'}) skipping: [localhost] =&gt; (item={'id': '88579cbc-9d5d-462f-8816-635901b6a12e', 'name': 'TestRoleAdmin', 'composite': False, 'clientRole': True, 'containerId': 'f084b840-c30d-4c93-933e-18f8be1ed19a'}) TASK [keycloak_realm : Get User TestAdmin] ************************************* ok: [localhost] TASK [keycloak_realm : Refresh keycloak auth token] **************************** ok: [localhost] TASK [keycloak_realm : Manage Client Role Mapping for TestAdmin] *************** included: /work/roles/keycloak_realm/tasks/manage_user_client_roles.yml for localhost =&gt; (item={'client': 'TestClient', 'role': 'TestRoleUser', 'realm': 'TestRealm'}) included: /work/roles/keycloak_realm/tasks/manage_user_client_roles.yml for localhost =&gt; (item={'client': 'TestClient', 'role': 'TestRoleAdmin', 'realm': 'TestRealm'}) TASK [keycloak_realm : Get Realm for role] ************************************* ok: [localhost] TASK [keycloak_realm : Check if Mapping is available] ************************** ok: [localhost] TASK [keycloak_realm : Create Role Mapping] ************************************ ok: [localhost] =&gt; (item={'id': '5cdd02f6-8341-4cd0-ba31-46631a847cdf', 'name': 'TestRoleUser', 'composite': False, 'clientRole': True, 'containerId': 'f084b840-c30d-4c93-933e-18f8be1ed19a'}) skipping: [localhost] =&gt; (item={'id': '88579cbc-9d5d-462f-8816-635901b6a12e', 'name': 'TestRoleAdmin', 'composite': False, 'clientRole': True, 'containerId': 'f084b840-c30d-4c93-933e-18f8be1ed19a'}) TASK [keycloak_realm : Get Realm for role] ************************************* ok: [localhost] TASK [keycloak_realm : Check if Mapping is available] ************************** ok: [localhost] TASK [keycloak_realm : Create Role Mapping] ************************************ ok: [localhost] =&gt; (item={'id': '88579cbc-9d5d-462f-8816-635901b6a12e', 'name': 'TestRoleAdmin', 'composite': False, 'clientRole': True, 'containerId': 'f084b840-c30d-4c93-933e-18f8be1ed19a'}) PLAY RECAP ********************************************************************* localhost : ok=82 changed=16 unreachable=0 failed=0 skipped=14 rescued=2 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the playbook has run successfully, you can verify on the target instance that the SSO server is running (as a &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; service) using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; # systemctl status keycloak ● keycloak.service - keycloak Server Loaded: loaded (/etc/systemd/system/keycloak.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2022-12-28 14:24:28 UTC; 26min ago Process: 1607 ExecStop=/opt/keycloak/keycloak-service.sh stop (code=exited, status=0/SUCCESS) Process: 1627 ExecStart=/opt/keycloak/keycloak-service.sh start (code=exited, status=0/SUCCESS) Main PID: 1742 (java) CGroup: /system.slice/keycloak.service ├─1630 /bin/sh /opt/keycloak/keycloak-18.0.2/bin/standalone.sh -Djboss.bind.address=0.0.0.0 -Djboss.http.port=8080 -Djboss.https.port=8443 -Djboss.management.http.port=9990 -Djbo&gt; └─1742 /usr/lib/jvm/java-11-openjdk-11.0.17.0.8-2.el8_6.x86_64/bin/java -D[Standalone] -server -Xms1024m -Xmx2048m --add-exports=java.desktop/sun.awt=ALL-UNNAMED --add-exports=ja&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,360 INFO [org.jboss.resteasy.resteasy_jaxrs.i18n] (ServerService Thread Pool -- 59) RESTEASY002220: Adding singleton resour&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,360 INFO [org.jboss.resteasy.resteasy_jaxrs.i18n] (ServerService Thread Pool -- 59) RESTEASY002220: Adding singleton resour&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,360 INFO [org.jboss.resteasy.resteasy_jaxrs.i18n] (ServerService Thread Pool -- 59) RESTEASY002220: Adding singleton resour&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,360 INFO [org.jboss.resteasy.resteasy_jaxrs.i18n] (ServerService Thread Pool -- 59) RESTEASY002210: Adding provider singlet&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,428 INFO [org.wildfly.extension.undertow] (ServerService Thread Pool -- 59) WFLYUT0021: Registered web context: '/auth' for&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,487 INFO [org.jboss.as.server] (ServerService Thread Pool -- 42) WFLYSRV0010: Deployed "keycloak-server.war" (runtime-name &gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,522 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming server Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,524 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: Keycloak 18.0.2 (WildFly Core 18.1.1.Final) started in 8112ms&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,526 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/&gt; Dec 28 14:24:35 515ef9b313a5 keycloak-service.sh[1742]: 14:24:35,527 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To go even further, we can add a check to our playbook that will use the Keycloak admin credentials to get a token from the SSO server. This emulates what will happen when a user tries to access an application using the SSO service. Thus, if it works fine, it confirms the service is functional:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- name: Verify token api call ansible.builtin.uri: url: "{{ keycloak_port }}/auth/realms/master/protocol/openid-connect/token" method: POST body: "client_id=admin-cli&amp;username=admin&amp;password={{ keycloak_admin_password }}&amp;grant_type=password" validate_certs: no register: keycloak_auth_response until: keycloak_auth_response.status == 200 retries: 2 delay: 2 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;On top of deploying the Keycloak server, we have fully automated the configuration of our SSO. We can deploy a fully functional instance, in any environment, without any manual intervention. Most importantly, it is accomplished in a secure and repeatable fashion. With just this playbook, you can set up the entire infrastructure for SSO in a matter of minutes using the tooling provided by the Ansible Middleware project.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/20/automate-your-sso-ansible-and-keycloak" title="Automate your SSO with Ansible and Keycloak"&gt;Automate your SSO with Ansible and Keycloak&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-02-20T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.16.3.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-3-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-3-final-released/</id><updated>2023-02-17T00:00:00Z</updated><published>2023-02-17T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.16.3.Final, our third maintenance release for the 2.16 release train. As usual, it contains bugfixes and documentation improvements, together with some small enhancements such as the ability to define custom credentials for the Flyway connection. It should be a safe upgrade for anyone already using 2.16....</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-02-17T00:00:00Z</dc:date></entry></feed>
